{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6ukXDdxjdP8"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -i https://test.pypi.org/simple/ coupledvae==0.0.14 -q"
      ],
      "metadata": {
        "id": "45Uv1fTlUcfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhpHDXYtjclf"
      },
      "outputs": [],
      "source": [
        "from coupledvae.VAEMNIST import VAE\n",
        "from coupledvae.experiment_utils import *\n",
        "from coupledvae.setup_funcs import *\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3u_Zb3RE7ut"
      },
      "source": [
        "# Mount GDRIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwtbFENnE79f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsyPIq0MIcEC"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZZ62PjekJvo"
      },
      "outputs": [],
      "source": [
        "# Random seed is the microsecond for current time.\n",
        "random_seed = datetime.now().microsecond\n",
        "\n",
        "# Set the training and testing batch sizes.\n",
        "BATCH_SIZE_TRAIN = 128\n",
        "BATCH_SIZE_TEST = 5000\n",
        "# The number at which to split training sets and validation set, with training \n",
        "# set size = mnist_split, and validation = 60000 - mnist_split.\n",
        "mnist_split = '55000' \n",
        "\n",
        "# List the different mnist data sets to use. The first should be the training\n",
        "# dataset.\n",
        "#corrupted_names = ['identity', 'motion_blur', 'shot_noise', 'spatter', 'fog']\n",
        "corrupted_names = ['identity', 'shot_noise']\n",
        "\n",
        "datasets_names = ['mnist'] + [\n",
        "  f'mnist_corrupted/{corrupted_name}' for corrupted_name in corrupted_names\n",
        "  ]\n",
        "\n",
        "# Download the data sets.\n",
        "datasets = get_datasets_(\n",
        "    datasets_names, \n",
        "    BATCH_SIZE_TRAIN, \n",
        "    BATCH_SIZE_TEST, \n",
        "    mnist_split, \n",
        "    random_seed\n",
        "    )\n",
        "\n",
        "training_datasets = ['train/']\n",
        "testing_datasets = ['test/' + name.split('/')[1] for name in datasets_names[1:]]\n",
        "#training_datasets = [datasets_names[0]]\n",
        "\n",
        "# Get the list of keys from the datasets dict.\n",
        "testing_datasets = list(datasets.keys())\n",
        "# Drop 'mnist', so only the corrupted dataset names remain.\n",
        "testing_datasets.remove('mnist')\n",
        "# Create an empty dictionary to hold only the testing datasets.\n",
        "testing_datasets_dict = dict()\n",
        "# Loop through the corrupted dataset names.\n",
        "for dataset in testing_datasets:\n",
        "  # Add the corrupted data set to the new dictionary.\n",
        "  testing_datasets_dict[dataset] = datasets[dataset]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0929v8fLIlQp"
      },
      "source": [
        "# Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEZGlQtm-jXl"
      },
      "outputs": [],
      "source": [
        "check_gpu_availibility()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yvQilJMlcH4"
      },
      "outputs": [],
      "source": [
        "###\n",
        "# VAE Initializing Parameters\n",
        "###\n",
        "\n",
        "# Latent dim, set the dimensionality of the latent space.\n",
        "z_dim_vals = [2]#[2, 4, 8, 16, 32]\n",
        "# Whether to use the analytical coupled divergence, or approximate.\n",
        "analytic_kl = True\n",
        "# Set the weight to place on the coupled dsivergence.\n",
        "beta = 1. # 1., 2., ..., 10.\n",
        "# Set the standard deviation of the prior distribution.\n",
        "p_std = 1.\n",
        "# Set the loss coupling.\n",
        "loss_coupling_vals = [0.5]#[1e-6, 0.025, 0.05, 0.075, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "# Set the number of base filters in the CNN.\n",
        "n_filter_base = 64\n",
        "# Set the learning rate for the Adam optimizer.\n",
        "learning_rate = 0.0005\n",
        "\n",
        "\n",
        "###\n",
        "# VAE Training Parameters\n",
        "###\n",
        "\n",
        "# Set the number of epochs to display.\n",
        "n_epoch = 150\n",
        "# Set the number of epochs before plots are displayed.\n",
        "n_epoch_display = 10\n",
        "# Whether or not to display plots while training.\n",
        "show_display = False\n",
        "display_sample = True\n",
        "\n",
        "\n",
        "###\n",
        "# Setting Paths\n",
        "###\n",
        "dataset_type = 'mnist'\n",
        "# Set the version of the code being run.\n",
        "version = 'v9_January_07_2023_MNIST'\n",
        "# Create the root path where the data will be stored.\n",
        "#save_path = Path(\n",
        "#    f'gdrive/My Drive/Colab Notebooks/coupled_vae/vae/output/{version}/'\n",
        "#    )\n",
        "save_path = Path(\n",
        "    f'gdrive/My Drive/Colab Notebooks/Coupled VAE Public/{version}/'\n",
        "    )\n",
        "# If the path does not exist, make it.\n",
        "save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Set the dirctory where run results will be saved.\n",
        "model_path = save_path / str(random_seed)\n",
        "model_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create the folders for this run in the google drive. It will not override \n",
        "# existing version and seed folders\n",
        "create_gdrive_output_folders(model_path,\n",
        "                             img_folders=corrupted_names)\n",
        "\n",
        "# Save the parameters in a dict.\n",
        "param_dict = {\n",
        "  'random_seed': random_seed,\n",
        "  'z_dim_vals': z_dim_vals,\n",
        "  'analytic_kl': analytic_kl,\n",
        "  'beta': beta,\n",
        "  'p_std': p_std,\n",
        "  'loss_coupling_vals': loss_coupling_vals,\n",
        "  'n_filter_base': n_filter_base,\n",
        "  'learning_rate': learning_rate,\n",
        "  'n_epoch': n_epoch,\n",
        "  'n_epoch_display': n_epoch_display,\n",
        "  'train_batch_size': BATCH_SIZE_TRAIN,\n",
        "  'test_batch_size': BATCH_SIZE_TEST,\n",
        "  'val_split': mnist_split,\n",
        "  'datasets': datasets_names,\n",
        "  'show_display': show_display,\n",
        "  'display_sample': show_display,\n",
        "  'model_path': model_path\n",
        "}\n",
        "\n",
        "# Set the path for the experiment tracking CSV file.\n",
        "experiment_tracker_path = save_path / 'experiment_tracker.csv'\n",
        "# Update the file.\n",
        "update_experiments(param_dict, experiment_tracker_path)\n",
        "\n",
        "# Set the training and testing paths.\n",
        "training_path = model_path / 'train'\n",
        "testing_path = model_path / 'test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asZBIWqiUmZ1"
      },
      "source": [
        "# Train VAE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = 20\n",
        "\n",
        "vae_dict = train_VAEs(\n",
        "    loss_coupling_vals=loss_coupling_vals, \n",
        "    z_dim_vals=z_dim_vals,\n",
        "    n_filter_base=n_filter_base,\n",
        "    beta=beta,\n",
        "    p_std=p_std, \n",
        "    analytic_kl=analytic_kl, \n",
        "    n_epoch=1,# TODO n_epoch,\n",
        "    n_epoch_display=n_epoch_display, \n",
        "    datasets=datasets,\n",
        "    dataset_type=dataset_type,\n",
        "    datasets_names=training_datasets,\n",
        "    random_seed=random_seed, \n",
        "    model_path=training_path,\n",
        "    show_display=show_display,\n",
        "    early_stop=early_stop,\n",
        "    cvae_type='MNIST'\n",
        "    )"
      ],
      "metadata": {
        "id": "HxVP6t4qj3DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgxL7BoGGT4B"
      },
      "source": [
        "# Plot Training Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr1A1yYx3eYu"
      },
      "outputs": [],
      "source": [
        "# Plot the latent space.\n",
        "if z_dim_vals == [2]:\n",
        "  for vae_key in vae_dict.keys():\n",
        "    print(f'Latent Space for {vae_key}')\n",
        "    plot_latent_images(vae_dict[vae_key].model, n=15, digit_size=28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joC0eJDJNAwA"
      },
      "outputs": [],
      "source": [
        "plot_training(vae_dict, metric='neg_elbo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ozIkzP5MyNw"
      },
      "outputs": [],
      "source": [
        "plot_training(vae_dict, metric='recon_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pur9pNROMzCc"
      },
      "outputs": [],
      "source": [
        "plot_training(vae_dict, metric='coupled_div')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydKFBgXCf5Z4"
      },
      "outputs": [],
      "source": [
        "best_model_epoch = vae_dict[''].val_metrics_df.loc[\n",
        "  vae_dict[''].val_metrics_df['val_neg_elbo'] == vae_dict[''].val_metrics_df['val_neg_elbo'].min()\n",
        "].index.values + 1\n",
        "\n",
        "print(f'The best model was saved at epoch {best_model_epoch[0]}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxGq2nAOGXOq"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-2PJr9Qo_1m"
      },
      "outputs": [],
      "source": [
        "vae = vae_dict['']\n",
        "vae = VAE(z_dim=vae.__dict__['z_dim'], \n",
        "          beta=vae.__dict__['beta'], \n",
        "          p_std=vae.__dict__['p_std'], \n",
        "          loss_coupling=vae.__dict__['loss_coupling'],\n",
        "          analytic_kl=vae.__dict__['analytic_kl'], \n",
        "          dtype=vae.__dict__['dtype'], \n",
        "          display_path=vae.__dict__['display_path']\n",
        ")\n",
        "\n",
        "# Load the best model by validation set performance from the checkpoints.\n",
        "vae.model.load_weights(str(model_path) + '/train/cp.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cH7Irj6K2KYO"
      },
      "outputs": [],
      "source": [
        "# Get the list of keys from the datasets dict.\n",
        "testing_datasets = list(datasets.keys())\n",
        "# Drop 'mnist', so only the corrupted dataset names remain.\n",
        "testing_datasets.remove('mnist')\n",
        "# Create an empty dictionary to hold only the testing datasets.\n",
        "testing_datasets_dict = dict()\n",
        "# Loop through the corrupted dataset names.\n",
        "for dataset in testing_datasets:\n",
        "  # Add the corrupted data set to the new dictionary.\n",
        "  testing_datasets_dict[dataset] = datasets[dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yukb92gD329B"
      },
      "outputs": [],
      "source": [
        "test_VAE_loop(\n",
        "    my_vae=vae,\n",
        "    datasets=testing_datasets_dict, \n",
        "    test_path=testing_path, \n",
        "    show_display=True,\n",
        "    random_seed=random_seed,\n",
        "    test_coupling=1e-6\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#January 07, 2023\n",
        "#Assume run has finished successfully.\n",
        "#Grab a image at random. \n",
        "#Produce 3 realizations of z  (1) Get rid of seed=0 in Sampler (2) pass in \n",
        "#vae is the instance of the object\n",
        "\n",
        "#Decode and exhibit them."
      ],
      "metadata": {
        "id": "d9QXLq0yZP6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "catdnZqha9kB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}