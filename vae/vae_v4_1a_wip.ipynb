{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vae_v4.1a_wip.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEJjn06qaEDP"
      },
      "source": [
        "#### **1) Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPyJih55aEDQ",
        "outputId": "1866d6dc-f089-4664-870b-aeb9fa864dd8"
      },
      "source": [
        "# Don't forget to restart\n",
        "# !pip install -U tensorflow_probability -q\n",
        "!pip install ipdb tensorflow==2.5.0 -q\n",
        "!pip install git+https://github.com/tensorflow/docs -q"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_sk90OFaEDT"
      },
      "source": [
        "from IPython import display\n",
        "\n",
        "import ipdb\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras.layers import Input, InputLayer, Lambda, Reshape, Dropout, \\\n",
        "                                    Flatten, Dense, Conv2D, Conv2DTranspose\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH0o4f7xUfUQ",
        "outputId": "a2cddd98-b27a-4665-ca3b-38086c72b7e0"
      },
      "source": [
        "pip list | grep tensorflow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow                    2.5.0                                         \n",
            "tensorflow-datasets           4.0.1                                         \n",
            "tensorflow-docs               0.0.07d6de2784363a81e65d5af245dc41b3e71eb65b5-\n",
            "tensorflow-estimator          2.5.0                                         \n",
            "tensorflow-gcs-config         2.5.0                                         \n",
            "tensorflow-hub                0.12.0                                        \n",
            "tensorflow-metadata           1.0.0                                         \n",
            "tensorflow-probability        0.12.1                                        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1M4C3hA88oN",
        "outputId": "50eee941-6fdf-49ca-cffa-2c30a23cc0b0"
      },
      "source": [
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "    print('WARNING: GPU device not found.')\n",
        "else:\n",
        "    print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SUCCESS: Found GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTEQ7kMIQ8cg",
        "outputId": "3a6ef049-9d10-43f2-e2b5-0d1fac23b3b9"
      },
      "source": [
        "# List all physical devices\n",
        "tf.config.list_physical_devices()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
              " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1m4X6DJGb52"
      },
      "source": [
        "If using Google Colab, save in your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcpWSzBBGayV",
        "outputId": "48f00d95-a9e3-4756-d01c-c82658041610"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RKA0uShaEDW"
      },
      "source": [
        "#### **2) Extract Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63HS2FEDhIVY"
      },
      "source": [
        "# BATCH_SIZE = 32\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZEdso5kQDb7"
      },
      "source": [
        "##### **2b) Fashion MNIST Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVXMpXhIQ-tN"
      },
      "source": [
        "datasets, datasets_info = tfds.load(name='fashion_mnist',\n",
        "                                    with_info=True,\n",
        "                                    as_supervised=False\n",
        "                                    )"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsgYr0XIQZ46"
      },
      "source": [
        "def _preprocess(sample):\n",
        "    image = tf.cast(sample['image'], tf.float32) / 255.  # Scale to unit interval.\n",
        "    # image = tf.where(image > .5, 1.0, 0.0)\n",
        "    return image"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlYKoRPpQZ7s"
      },
      "source": [
        "fm_train_dataset = (datasets['train']\n",
        "                    .map(_preprocess)\n",
        "                   .batch(BATCH_SIZE)\n",
        "                   .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "                   .shuffle(60000)\n",
        "                   )\n",
        "\n",
        "fm_test_dataset = (datasets['test']\n",
        "                  .map(_preprocess)\n",
        "                  .batch(BATCH_SIZE)\n",
        "                  .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "                  )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "ucp5hH6CR7vl",
        "outputId": "2b2480f2-43f8-4f79-d276-be62b7889735"
      },
      "source": [
        "for train_batch in fm_train_dataset.take(1):\n",
        "    image = train_batch[0].numpy()\n",
        "image = np.squeeze(image, axis=2)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(image, cmap='gray')\n",
        "# plt.title('corresponding image label {}'.format(label))\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.axis('off')\n",
        "plt.show();"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zopEYyzVZynY"
      },
      "source": [
        "#### **3) Model Class**\n",
        "\n",
        "Credit: https://www.tensorflow.org/api_docs/python/tf/keras/datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QJN4AtF5kpP"
      },
      "source": [
        "from tensorflow.keras import layers as tfkl"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5i8DixeY8kG"
      },
      "source": [
        "class Sampler_Z(tfkl.Layer):\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean, logvar = inputs\n",
        "        # Reparameterize\n",
        "        eps = tf.random.normal(shape=mean.shape)\n",
        "        z_sample = eps * tf.exp(logvar * .5) + mean\n",
        "        return z_sample"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UudCMLdsxNV"
      },
      "source": [
        "# Encoder/Decoder Model 3 (Fashion MNIST)\n",
        "class EncoderZ_3(tfk.Model):\n",
        "\n",
        "    def __init__(self, latent_dim, n_filter_base, name=\"encoder\", **kwargs):\n",
        "        super(EncoderZ_3, self).__init__(name=name, **kwargs)\n",
        "        # Block-1\n",
        "        self.conv_layer_1 = tfkl.Conv2D(filters=n_filter_base, kernel_size=3,\n",
        "                                        strides=1, padding='same', name='conv_1'\n",
        "                                        )\n",
        "        self.batch_layer_1 = tfkl.BatchNormalization(name='bn_1')\n",
        "        self.activation_layer_1 = tfkl.Activation(tf.nn.leaky_relu, name='lrelu_1')\n",
        "        # Block-2\n",
        "        self.conv_layer_2 = tfkl.Conv2D(filters=n_filter_base*2, kernel_size=3,\n",
        "                                        strides=2, padding='same', name='conv_2'\n",
        "                                        )\n",
        "        self.batch_layer_2 = tfkl.BatchNormalization(name='bn_2')\n",
        "        self.activation_layer_2 = tfkl.Activation(tf.nn.leaky_relu, name='lrelu_2')\n",
        "        # Block-3\n",
        "        self.conv_layer_3 = tfkl.Conv2D(filters=n_filter_base*2, kernel_size=3,\n",
        "                                        strides=2, padding='same', name='conv_3'\n",
        "                                        )\n",
        "        self.batch_layer_3 = tfkl.BatchNormalization(name='bn_3')\n",
        "        self.activation_layer_3 = tfkl.Activation(tf.nn.leaky_relu, name='lrelu_3')\n",
        "        # Block-4\n",
        "        self.conv_layer_4 = tfkl.Conv2D(filters=n_filter_base*2, kernel_size=3,\n",
        "                                        strides=1, padding='same', name='conv_4'\n",
        "                                        )\n",
        "        self.batch_layer_4 = tfkl.BatchNormalization(name='bn_4')\n",
        "        self.activation_layer_4 = tfkl.Activation(tf.nn.leaky_relu, name='lrelu_4')\n",
        "        # Final Block\n",
        "        self.flatten_layer = Flatten()\n",
        "        self.dense_mean = Dense(latent_dim, activation=None, name='z_mean')\n",
        "        self.dense_raw_stddev = Dense(latent_dim, activation=None,\n",
        "                                      name='z_raw_stddev'\n",
        "                                      )\n",
        "        self.sampler_z = Sampler_Z()\n",
        "\n",
        "    # Functional\n",
        "    def call(self, x_input, training=False):\n",
        "        z = self.conv_layer_1(x_input)\n",
        "        z = self.batch_layer_1(z)\n",
        "        z = self.activation_layer_1(z)\n",
        "        z = self.conv_layer_2(z)\n",
        "        z = self.batch_layer_2(z)\n",
        "        z = self.activation_layer_2(z)\n",
        "        z = self.conv_layer_3(z)\n",
        "        z = self.batch_layer_3(z)\n",
        "        z = self.activation_layer_3(z)\n",
        "        z = self.conv_layer_4(z)\n",
        "        z = self.batch_layer_4(z)\n",
        "        z = self.activation_layer_4(z)\n",
        "        z = self.flatten_layer(z)\n",
        "        mean = self.dense_mean(z)\n",
        "        logvar = self.dense_raw_stddev(z)\n",
        "        z_sample = self.sampler_z((mean, logvar))\n",
        "        return z_sample, mean, logvar\n",
        "\n",
        "\n",
        "class DecoderX_3(tfk.Model):\n",
        "\n",
        "    def __init__(self, latent_dim, n_filter_base, name=\"decoder\", **kwargs):\n",
        "        super(DecoderX_3, self).__init__(name=name, **kwargs)\n",
        "        # For MNIST / Fashion MNIST images\n",
        "        self.dense_z_input = tfkl.Dense(units=7*7*n_filter_base*2,\n",
        "                                        activation=tf.nn.relu\n",
        "                                        )\n",
        "        self.reshape_layer = tfkl.Reshape(target_shape=(7, 7, n_filter_base*2))\n",
        "        # For 0xflower images\n",
        "        # self.dense_z_input = tfkl.Dense(units=56*56*n_filter_base*2,\n",
        "        #                                 activation=tf.nn.relu\n",
        "        #                                 )\n",
        "        # self.reshape_layer = tfkl.Reshape(target_shape=(56, 56, n_filter_base*2))\n",
        "        # Block-1\n",
        "        self.conv_transpose_layer_1 = tfkl.Conv2DTranspose(filters=n_filter_base*2,\n",
        "                                                           kernel_size=3,\n",
        "                                                           strides=1, \n",
        "                                                           padding='same',\n",
        "                                                           name='conv_transpose_1'\n",
        "                                                           )\n",
        "        self.batch_layer_1 = tfkl.BatchNormalization(name='bn_1')\n",
        "        self.activation_layer_1 = tfkl.Activation(tf.nn.leaky_relu, name='lrelu_1')\n",
        "        # Block-2\n",
        "        self.conv_transpose_layer_2 = tfkl.Conv2DTranspose(filters=n_filter_base*2,\n",
        "                                                           kernel_size=3,\n",
        "                                                           strides=2, \n",
        "                                                           padding='same',\n",
        "                                                           name='conv_transpose_2'\n",
        "                                                           )\n",
        "        self.batch_layer_2 = tfkl.BatchNormalization(name='bn_2')\n",
        "        self.activation_layer_2 = tfkl.Activation(tf.nn.leaky_relu, name='lrelu_2')\n",
        "        # Block-3\n",
        "        self.conv_transpose_layer_3 = tfkl.Conv2DTranspose(filters=n_filter_base,\n",
        "                                                           kernel_size=3,\n",
        "                                                           strides=2, \n",
        "                                                           padding='same',\n",
        "                                                           name='conv_transpose_3'\n",
        "                                                           )\n",
        "        self.batch_layer_3 = tfkl.BatchNormalization(name='bn_3')\n",
        "        self.activation_layer_3 = tfkl.Activation(tf.nn.leaky_relu, name='lrelu_3')\n",
        "        # Block-4\n",
        "        # For MNIST / Fashion MNIST images\n",
        "        self.conv_transpose_layer_4 = tfkl.Conv2DTranspose(filters=1,\n",
        "                                                           kernel_size=3,\n",
        "                                                           strides=1, \n",
        "                                                           padding='same',\n",
        "                                                           name='conv_transpose_4'\n",
        "                                                           )\n",
        "        # For 0xflower images\n",
        "        # self.conv_transpose_layer_4 = tfkl.Conv2DTranspose(filters=3,\n",
        "        #                                                    kernel_size=3,\n",
        "        #                                                    strides=1, \n",
        "        #                                                    padding='same',\n",
        "        #                                                    name='conv_transpose_4'\n",
        "        #                                                    )\n",
        "\n",
        "    # Functional\n",
        "    def call(self, z, training=False):\n",
        "        x_output = self.dense_z_input(z)\n",
        "        x_output = self.reshape_layer(x_output)\n",
        "        x_output = self.conv_transpose_layer_1(x_output)\n",
        "        x_output = self.batch_layer_1(x_output)\n",
        "        x_output = self.activation_layer_1(x_output)\n",
        "        x_output = self.conv_transpose_layer_2(x_output)\n",
        "        x_output = self.batch_layer_2(x_output)\n",
        "        x_output = self.activation_layer_2(x_output)\n",
        "        x_output = self.conv_transpose_layer_3(x_output)\n",
        "        x_output = self.batch_layer_3(x_output)\n",
        "        x_output = self.activation_layer_3(x_output)\n",
        "        x_output = self.conv_transpose_layer_4(x_output)\n",
        "        return x_output"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q5aKhDpKkhl"
      },
      "source": [
        "class VAEModel(tfk.Model):\n",
        "    \"\"\"Convolutional variational autoencoder base model.\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, latent_dim, n_filter_base, **kwargs):\n",
        "        super(VAEModel, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = tfk.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tfk.metrics.Mean(\n",
        "            name=\"reconstruction_loss\"\n",
        "        )\n",
        "        self.kl_loss_tracker = tfk.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    @tf.function\n",
        "    def sample(self, z_sample):\n",
        "        # z_sample, _, _ = self.encoder(x_sample)\n",
        "        x_recons_logits = self.decoder(z_sample)\n",
        "        sample_images = tf.sigmoid(x_recons_logits)  # predictions\n",
        "        # sample_images = tf.math.sigmoid(x_recons_logits)  # predictions\n",
        "        return sample_images\n",
        "\n",
        "    # [KC] Problem 3:\n",
        "    def train_step(self, data):\n",
        "        \"\"\"Executes one training step and returns the loss.\n",
        "\n",
        "        This function computes the loss and gradients, and uses the latter to\n",
        "        update the model's parameters.\n",
        "        \"\"\"\n",
        "        # # Pick a sample of the test set for generating output images\n",
        "        # # num_examples_to_generate = n_sample\n",
        "        # assert batch_size >= n_sample\n",
        "        # for test_batch in test_dataset.take(1):\n",
        "        #     test_sample = test_batch[0:n_sample, :, :, :]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.compute_loss(x_true)\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     z_mean, z_log_var, z = self.encoder(data)\n",
        "        #     reconstruction = self.decoder(z)\n",
        "        #     reconstruction_loss = tf.reduce_mean(\n",
        "        #         tf.reduce_sum(\n",
        "        #             keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
        "        #         )\n",
        "        #     )\n",
        "        #     kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        #     kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "        #     total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        # Need to use test data\n",
        "        self.generate_and_save_images(epoch, test_sample)\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def compute_loss(self, x_true):\n",
        "        x_recons_logits, z_sample, mean, logvar = self.model(x_true)\n",
        "        # Cross Entropy Loss \n",
        "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_recons_logits,\n",
        "                                                            labels=x_true\n",
        "                                                            )\n",
        "        # Negative Log-Likelihood\n",
        "        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "        neg_ll = logpx_z\n",
        "        # KL-Divergence\n",
        "        logpz = self.log_normal_pdf(z_sample, 0., 1.)\n",
        "        logqz_x = self.log_normal_pdf(z_sample, mean, logvar)\n",
        "        kl_div = logqz_x - logpz\n",
        "        # ELBO\n",
        "        elbo = tf.math.reduce_mean(-kl_div + neg_ll)\n",
        "        return -elbo\n",
        "\n",
        "    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
        "        log2pi = tf.math.log(2. * np.pi)\n",
        "        return tf.reduce_sum(\n",
        "            -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "            axis=raxis)\n",
        "\n",
        "    # [KC] Problem 4:\n",
        "    def generate_and_save_images(self, epoch, test_sample):\n",
        "        z_sample, _, _ = self.model.encoder(test_sample)\n",
        "        decoder_sample_images = self.model.sample(z_sample)  # predictions\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "        for i in range(decoder_sample_images.shape[0]):\n",
        "            plt.subplot(4, 4, i + 1)\n",
        "            plt.imshow(decoder_sample_images[i, :, :, 0], cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "        # tight_layout minimizes the overlap between 2 sub-plots\n",
        "        plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "        plt.show()\n",
        "\n",
        "    '''\n",
        "    def call(self, x_input):\n",
        "        z_sample, mean, logvar = self.encoder(x_input)\n",
        "        x_recons_logits = self.decoder(z_sample)\n",
        "        return x_recons_logits, z_sample, mean, logvar\n",
        "    '''\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_DtMMuTv91K"
      },
      "source": [
        "'''\n",
        "class VAE:\n",
        "    \"\"\"Variational Autoencoder wrapper.\"\"\"\n",
        "    def __init__(self, image_dim, z_dim, n_filter_base=32, learning_rate=0.0005):\n",
        "        self.optimizer = tfk.optimizers.Adam(learning_rate)\n",
        "        self.loss_metric = tfk.metrics.Mean()\n",
        "        self.model = VAEModel(z_dim, n_filter_base)\n",
        "\n",
        "    def train(self, train_dataset, test_dataset, batch_size=0, \n",
        "              n_epoch=10, n_sample=4):\n",
        "        # Pick a sample of the test set for generating output images\n",
        "        # num_examples_to_generate = n_sample\n",
        "        assert batch_size >= n_sample\n",
        "        for test_batch in test_dataset.take(1):\n",
        "            test_sample = test_batch[0:n_sample, :, :, :]\n",
        "\n",
        "        self.generate_and_save_images(0, test_sample)\n",
        "        for epoch in range(1, n_epoch + 1):\n",
        "            start_time = time.time()\n",
        "            for train_x in tqdm(train_dataset):\n",
        "                self.train_step(train_x)\n",
        "            end_time = time.time()\n",
        "\n",
        "            loss = tfk.metrics.Mean()\n",
        "            for test_x in test_dataset:\n",
        "                loss(self.compute_loss(test_x))\n",
        "            elbo = -loss.result()\n",
        "            display.clear_output(wait=False)\n",
        "            print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
        "                  .format(epoch, elbo, end_time - start_time)\n",
        "                  )\n",
        "            self.generate_and_save_images(epoch, test_sample)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, x_true):\n",
        "        \"\"\"Executes one training step and returns the loss.\n",
        "\n",
        "        This function computes the loss and gradients, and uses the latter to\n",
        "        update the model's parameters.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = self.compute_loss(x_true)\n",
        "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "\n",
        "    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
        "        log2pi = tf.math.log(2. * np.pi)\n",
        "        return tf.reduce_sum(\n",
        "            -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "            axis=raxis)\n",
        "\n",
        "    def compute_loss(self, x_true):\n",
        "        x_recons_logits, z_sample, mean, logvar = self.model(x_true)\n",
        "        # Cross Entropy Loss \n",
        "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_recons_logits,\n",
        "                                                            labels=x_true\n",
        "                                                            )\n",
        "        # Negative Log-Likelihood\n",
        "        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "        neg_ll = logpx_z\n",
        "        # KL-Divergence\n",
        "        logpz = self.log_normal_pdf(z_sample, 0., 1.)\n",
        "        logqz_x = self.log_normal_pdf(z_sample, mean, logvar)\n",
        "        kl_div = logqz_x - logpz\n",
        "        # ELBO\n",
        "        elbo = tf.math.reduce_mean(-kl_div + neg_ll)\n",
        "        return -elbo\n",
        "\n",
        "    def generate_and_save_images(self, epoch, test_sample):\n",
        "        z_sample, _, _ = self.model.encoder(test_sample)\n",
        "        decoder_sample_images = self.model.sample(z_sample)  # predictions\n",
        "        fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "        for i in range(decoder_sample_images.shape[0]):\n",
        "            plt.subplot(4, 4, i + 1)\n",
        "            plt.imshow(decoder_sample_images[i, :, :, 0], cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "        # tight_layout minimizes the overlap between 2 sub-plots\n",
        "        plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "        plt.show()\n",
        "'''"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPBEVeMUyq6s"
      },
      "source": [
        "#### **4) Models Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoECtqtvqCxn",
        "outputId": "2022e631-8686-42d5-d9b3-77ef88c6f266"
      },
      "source": [
        "fm_train_dataset"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ShuffleDataset shapes: (None, 28, 28, 1), types: tf.float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMKOS-wB2GKh"
      },
      "source": [
        "# vae init params\n",
        "input_dim = 28  # image dim\n",
        "image_dim = (28, 28, 1)\n",
        "# image_dim = (224, 224, 3)\n",
        "z_dim = 2  # latent dim, set the dimensionality of the latent space to a plane for visualization later\n",
        "n_filter_base = 32  # number of base filters in the CNN (a lot of filters?)\n",
        "learning_rate = 0.0005  # uses for the internal Adam opt\n",
        "\n",
        "# vae.train params\n",
        "n_epoch = 10  # number of epochs\n",
        "batch_size = BATCH_SIZE\n",
        "show_display = True\n",
        "display_sample = False\n",
        "n_sample = 16  # number of example outputs to sample and generate, also n_display"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "_c-B4PqSHQgS",
        "outputId": "04f47e7b-7795-40ac-cac4-c57ec143395d"
      },
      "source": [
        "# Error 1: \n",
        "encoder = EncoderZ_3(z_dim, n_filter_base)\n",
        "encoder.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-3cbda80c2eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderZ_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_filter_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2475\u001b[0m     \"\"\"\n\u001b[1;32m   2476\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2477\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2478\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2479\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdPmtOlT8tPR"
      },
      "source": [
        "decoder = DecoderX_3(z_dim, n_filter_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHe9SKZJ2DGM"
      },
      "source": [
        "vae = VAEModel(encoder, decoder, image_dim, z_dim)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAMINvXoHmsB"
      },
      "source": [
        "vae.compile(optimizer=tfk.optimizers.Adam())"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNmYhKy_LBmO",
        "outputId": "aa4f1ebc-4749-4b07-ddb9-e4a01f7ecb0a"
      },
      "source": [
        "# [KC] Problem 2:\n",
        "type(fm_train_dataset), type(fm_test_dataset)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensorflow.python.data.ops.dataset_ops.ShuffleDataset,\n",
              " tensorflow.python.data.ops.dataset_ops.PrefetchDataset)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqM8nn4LH2p7",
        "outputId": "798b8b1d-062a-43bd-c261-41ac536d3b84"
      },
      "source": [
        "# What Keras does\n",
        "(x_train, _), (x_test, _) = tfk.datasets.mnist.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnX5kJjZH51b",
        "outputId": "b3fb711c-89c4-4016-ff03-61325f06222f"
      },
      "source": [
        "type(x_train), type(mnist_digits)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, numpy.ndarray)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "clKTIaOSHq1J",
        "outputId": "b5a67bde-c7d0-4881-ee65-c471dc563f42"
      },
      "source": [
        "vae.fit(fm_train_dataset, epochs=30, batch_size=128)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c80915a90b48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    763\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 764\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *\n        return step_function(self, iterator)\n    <ipython-input-14-eb727dc85fdf>:55 call  *\n        z_sample = self.sampler_z((mean, logvar))\n    <ipython-input-13-95c78c7a37ba>:6 call  *\n        eps = tf.random.normal(shape=mean.shape)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/random_ops.py:90 random_normal\n        shape_tensor = tensor_util.shape_tensor(shape)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py:1080 shape_tensor\n        return ops.convert_to_tensor(shape, dtype=dtype, name=\"shape\")\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:356 _tensor_shape_tensor_conversion_function\n        \"Cannot convert a partially known TensorShape to a Tensor: %s\" % s)\n\n    ValueError: Cannot convert a partially known TensorShape to a Tensor: (None, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgKvFXIY4Rcr"
      },
      "source": [
        "# %%time\n",
        "# vae.train(fm_train_dataset, fm_test_dataset, batch_size, n_epoch, n_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Iy6-7zLHt-r"
      },
      "source": [
        "#### **5) Models Display**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M0id04neWCA"
      },
      "source": [
        "def display_image(epoch_no):\n",
        "    return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmrpeyKmHtk-"
      },
      "source": [
        "plt.imshow(display_image(n_epoch))\n",
        "plt.axis('off')  # Display images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKoCFE7EH7AM"
      },
      "source": [
        "anim_file = 'cvae.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "    filenames = glob.glob('image*.png')\n",
        "    filenames = sorted(filenames)\n",
        "    for filename in filenames:\n",
        "        image = imageio.imread(filename)\n",
        "        writer.append_data(image)\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-iaj_akH7DY"
      },
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(anim_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WAUzHTjIC12"
      },
      "source": [
        "def plot_latent_images(model, n, digit_size=28):\n",
        "    \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n",
        "    norm = tfp.distributions.Normal(0, 1)\n",
        "    grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
        "    grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
        "    image_width = digit_size*n\n",
        "    image_height = image_width\n",
        "    image = np.zeros((image_height, image_width))\n",
        "\n",
        "    for i, yi in enumerate(grid_x):\n",
        "        for j, xi in enumerate(grid_y):\n",
        "            z = np.array([[xi, yi]])\n",
        "            x_decoded = model.sample(z)\n",
        "            digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n",
        "            image[i * digit_size: (i + 1) * digit_size,\n",
        "                j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(image, cmap='Greys_r')\n",
        "    plt.axis('Off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ-7JvLqIC9W"
      },
      "source": [
        "plot_latent_images(vae.model, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONCMXUZvIDAO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}