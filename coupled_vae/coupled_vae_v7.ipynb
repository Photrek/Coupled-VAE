{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ckeJMIQpMCW6"},"outputs":[],"source":["# Don't forget to restart\n","# !pip install -U tensorflow_probability -q\n","# !pip install ipdb tensorflow==2.5.0 -q\n","# !pip install ipdb -q\n","# !pip install git+https://github.com/tensorflow/docs -q\n","!pip install -U -i https://test.pypi.org/simple/ nsc -q"]},{"cell_type":"code","source":["def recon_loss(x_true, x_recons_logits, loss_coupling):\n","  p = x_true\n","  q = tf.math.sigmoid(x_recons_logits)\n","\n","  #Calculation of binary log_loss\n","  cross_ent_2 = p*nsc_tf.math.function.coupled_logarithm(\n","      q, \n","      kappa=loss_coupling) + (\n","          1-p\n","          )*nsc_tf.math.function.coupled_logarithm(\n","              1-q, \n","              kappa=loss_coupling\n","              )\n","\n","  logpx_z= tf.reduce_sum(cross_ent_2)\n","  neg_ll = -logpx_z\n","  return neg_ll\n","\n","x = tf.random.uniform([100])  # X ~ U(0, 1)\n","y = tf.math.log(x) - tf.math.log(1 - x)  # LOGITS\n","recon_loss(x, y, 0.0)"],"metadata":{"id":"mijDFGknEI1w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k6ukXDdxjdP8"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QhpHDXYtjclf"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","'''\n","Created on Fri Oct 22 00:41:39 2021\n","\n","@author: jkcle\n","'''\n","\n","from IPython import display\n","\n","import math\n","import glob\n","import imageio\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","import PIL\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import tensorflow_probability as tfp\n","import time\n","from datetime import datetime\n","import os\n","import random\n","from tqdm import tqdm\n","from collections import defaultdict\n","from sklearn.manifold import TSNE\n","\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","from tensorflow.keras.layers import Input, InputLayer, Lambda, Reshape, \\\n","                                    Dropout, Flatten, Dense, Conv2D, \\\n","                                    Conv2DTranspose\n","\n","import nsc_tf\n","from math import pi\n","\n","from tensorflow import repeat, squeeze, reduce_mean, matmul, expand_dims, \\\n","                       transpose, reduce_prod, pow, square, sqrt, cast, \\\n","                       where, zeros_like, int32, float32, equal, reduce_all \n","from tensorflow.random import set_seed\n","from tensorflow.math import log, lgamma, exp, is_finite, reduce_sum, \\\n","                            count_nonzero, add, subtract, multiply, divide\n","from tensorflow.linalg import det, diag_part, inv, trace"]},{"cell_type":"markdown","metadata":{"id":"Q3u_Zb3RE7ut"},"source":["# Additonal Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwtbFENnE79f"},"outputs":[],"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"bhUi3390ZDkk"},"source":["# Class and Function Definitions"]},{"cell_type":"markdown","metadata":{"id":"_2y2JXNJgN6Z"},"source":["## Generalized Mean Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ribpKb7CgWxZ"},"outputs":[],"source":["def kappa_to_r(kappa, dim=1):\n","  return -2*kappa / (1 + dim*kappa)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PESXAOYXgOT-"},"outputs":[],"source":["def generalized_mean(values: tf.Tensor, r: float = 1.0, weights: tf.Tensor= None) -> tf.Tensor:\n","    \"\"\"\n","    This function calculates the generalized mean of a 1-D array of non- \n","    negative real numbers using the coupled logarithm and exponential functions.\n","    \n","    Parameters\n","    ----------\n","    values : np.ndarray\n","        A 1-D numpy array (row vector) of non-negative numbers for which we are\n","        calculating the generalized mean.\n","    r : float, optional\n","        The risk bias and the power of the generalized mean. The default is 1.0 \n","        (Arithmetric Mean).\n","    weights : np.ndarray, optional\n","        A 1-D numpy array of the weights for each value. Default is None \n","        (which triggers a conditional to use equal weights).\n","    Returns\n","    -------\n","    gen_mean : float\n","        The coupled generalized mean.\n","    \"\"\"\n","            \n","    # If weights equals None, equally weight all observations.\n","    if type(weights) == type(None):\n","        weights = tf.ones(values.shape)\n","    \n","    # Calculate the log of the generalized mean by taking the dot product of the\n","    # weights vector and the vector of the coupled logarithm of the values and\n","    # divide the result by the sum of the the weights.\n","    if len(values.shape) > 1:\n","        log_gen_mean = (np.dot(weights, tf.transpose(nsc_tf.log(values, kappa=r, dim=0)))\n","                        / tf.reduce_sum(weights, axis=1))\n","        log_gen_mean = log_gen_mean[0]\n","    else:\n","        log_gen_mean = (np.dot(weights, nsc_tf.log(values, kappa=r, dim=0)) \n","                        / tf.reduce_sum(weights))\n","        \n","    # Calculate the generalized mean by exponentiating the log-generalized mean.\n","    gen_mean = nsc_tf.exp(log_gen_mean, kappa=r, dim=0)\n","    \n","    # Return the generalized mean.\n","    return gen_mean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcUwNkz1EcJU"},"outputs":[],"source":["\"\"\"\n","def generalized_mean(values, r):\n","    if r != 0:\n","      gen_mean = tf.pow(tf.reduce_sum(tf.divide(tf.pow(np.array(values), r), tf.cast(tf.size(values)), (1/r))))\n","    else:\n","      gen_mean = tf.exp(tf.reduce_sum(tf.divide(tf.math.log(values), tf.size(values))))\n","    return gen_mean\n","\"\"\"\n","def generalized_mean(values, r):\n","    if r != 0:\n","      gen_mean = tf.pow(\n","          tf.reduce_sum(\n","              tf.pow(np.array(values), r) * (1 / tf.size(values).numpy())\n","              ), \n","              (1/r)\n","          )\n","    else:\n","      gen_mean = tf.exp(tf.reduce_sum(tf.math.log(values) * (1 / tf.size(values).numpy())))\n","    return gen_mean"]},{"cell_type":"markdown","metadata":{"id":"9rU_nMnnjlKW"},"source":["## Set-Up Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqAKFRLpjlly"},"outputs":[],"source":["def update_experiments(experiment_params_dict, file_path):\n","  '''Update the CSV file that is tracking experimental parameters or make it\n","  if it does not exist.\n","\n","  Inputs\n","  ------\n","  experiment_params_dict : dict\n","    A dictionary containing experimental parameters.\n","  file_path : str\n","    A path to the file tracking experimental parameters.\n","\n","  Returns\n","  -------\n","  None\n","\n","  '''\n","\n","  # Covert the experiment_params_dict to a dataframe.\n","  experiment_params_df = pd.DataFrame(\n","      experiment_params_dict.items()\n","      ).set_index(0).T\n","  # If the file exists.\n","  if Path(file_path).exists():\n","    # Let the know the user know the file exists.\n","    print(f'Reading [{file_path}]')\n","    # Read in the file.\n","    old_experiments = pd.read_csv(file_path)\n","    # Let the user know the file is being updated.\n","    print(f'Updating [{file_path}]')\n","    # Add the new experimantal data to the old.\n","    experiment_params_df = pd.concat([old_experiments, experiment_params_df])\n","  # Let the user know the file is being written.\n","  print(f'Writing [{file_path}]')\n","  # Write the experimental params to the CSV.\n","  experiment_params_df.to_csv(file_path, index=False)\n","\n","  return\n","\n","\n","def create_gdrive_output_folders(save_path, \n","                                 img_folders=['identity', \n","                                              'motion_blur', \n","                                              'rotate', \n","                                              'translate'], # The first is the training dataset.\n","                                 viz_folders=['generated_images', \n","                                              'latent_spaces', \n","                                              'manifolds', \n","                                              'histograms',\n","                                              'metrics']\n","                                 ):\n","    save_path = Path(save_path)\n","    save_path.mkdir(parents=True, exist_ok=True)\n","\n","    train_img_path = save_path / 'train'\n","    train_img_path.mkdir(parents=True, exist_ok=True)\n","\n","    for viz_folder in viz_folders:\n","      viz_path = train_img_path / f'{viz_folder}'\n","      viz_path.mkdir(parents=True, exist_ok=True)\n","\n","    # Changed 8_26_2021: added explicit testing on each directory\n","    for img_folder in img_folders:\n","      img_path = save_path / 'test' / f'{img_folder}'\n","      img_path.mkdir(parents=True, exist_ok=True)\n","      for viz_folder in viz_folders:\n","        viz_path = img_path / f'{viz_folder}'\n","        viz_path.mkdir(parents=True, exist_ok=True)\n","\n","    return\n","\n","\n","def check_gpu_availibility():\n","  '''This function checks if a GPU is present in a Google Colab notebook.\n","  '''\n","\n","  # Import gpu_device_name from tensorflow.test.\n","  from tensorflow.test import gpu_device_name\n","  # If a GPU device is not detected, notify the user.\n","  if gpu_device_name() != '/device:GPU:0':\n","      print('WARNING: GPU device not found.')\n","  # If a GPU is detected, notify the user.\n","  else:\n","      print(f'SUCCESS: Found GPU: {gpu_device_name()}')\n","  return\n","\n","\n","def _preprocess(sample):\n","    '''Cast all the pixel values in a sample to 64-bit float and scale them\n","    between 0 and 1.\n","    '''\n","    image = tf.cast(sample['image'], tf.float64) / 255.\n","    return image\n","\n","\n","def _preprocess_label(sample):\n","    '''Cast the label of a sample to 64-bit integer.\n","    '''\n","    label = tf.cast(sample['label'], tf.int64)\n","    return label\n","\n","\n","def get_datasets(datasets_names, \n","                 batch_size_train, \n","                 batch_size_test, \n","                 mnist_split,\n","                 random_seed\n","                 ):\n","    datasets = defaultdict(dict)\n","    for datasets_name in datasets_names:\n","        print('============================================================')\n","        print(f'\\nExtracting {datasets_name.upper()} dataset...\\n')\n","        # # CW - *** modified code such that when MNIST is loaded it gets \n","        # # automatically split into train and validation sets\n","        if datasets_name == 'mnist':\n","          (mtrain, mvalidation, mtest), datasets_raw_info = tfds.load(\n","              name=datasets_name,\n","              with_info=True,\n","              as_supervised=False,\n","              split=[\n","                  'train[:'+mnist_split+']', \n","                  'train['+mnist_split+':]', \n","                  'test']\n","              )\n","          \n","          train_size = int(mnist_split)\n","          datasets[datasets_name]['train'] = (\n","              mtrain.map(_preprocess)\n","              .batch(batch_size_train)\n","              .prefetch(tf.data.experimental.AUTOTUNE)\n","              .shuffle(train_size, seed=random_seed)\n","              )\n","          \n","          datasets[datasets_name]['val'] = (\n","              mvalidation\n","              .map(_preprocess)\n","              .batch(batch_size_test)\n","              .prefetch(tf.data.experimental.AUTOTUNE)\n","              )\n","          datasets[datasets_name]['val_label'] = (\n","              mvalidation\n","              .map(_preprocess_label)\n","              .batch(batch_size_test)\n","              .prefetch(tf.data.experimental.AUTOTUNE)\n","              )\n","          datasets[datasets_name]['test'] = (\n","              mtest\n","              .map(_preprocess)\n","              .batch(batch_size_test)\n","              .prefetch(tf.data.experimental.AUTOTUNE)\n","              )\n","          datasets[datasets_name]['test_label'] = (\n","              mtest\n","              .map(_preprocess_label)\n","              .batch(batch_size_test)\n","              .prefetch(tf.data.experimental.AUTOTUNE)\n","              )\n","        else:\n","          datasets_raw, datasets_raw_info = tfds.load(\n","              name=datasets_name,\n","              with_info=True,\n","              as_supervised=False\n","              )\n","          datasets[datasets_name]['test'] = (\n","              datasets_raw['test']\n","              .map(_preprocess)\n","              .batch(batch_size_test)\n","              .prefetch(tf.data.experimental.AUTOTUNE)\n","              )\n","          datasets[datasets_name]['test_label'] = (\n","              datasets_raw['test']\n","              .map(_preprocess_label)\n","              .batch(batch_size_test)\n","              .prefetch(tf.data.experimental.AUTOTUNE)\n","              )\n","        print(datasets_raw_info)\n","\n","        print('Here')\n","        if 'corrupted' in datasets_name:\n","          # View some examples from the dataset\n","          fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n","          fig.subplots_adjust(hspace=0.2, wspace=0.1)\n","          for i, (elem, ax) in enumerate(\n","                  zip(datasets_raw['train'], axes.flat)\n","                  ):\n","              image = tf.squeeze(elem['image'])\n","              # print(image)\n","              label = elem['label']\n","\n","              ax.imshow(image, cmap='gray')\n","              ax.text(0.7, -0.12, f'Digit = {label}', ha='right',\n","                      transform=ax.transAxes, color='black')\n","              ax.set_xticks([])\n","              ax.set_yticks([])\n","              # plt.show()\n","        else:\n","          # View some examples from the dataset\n","          fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n","          fig.subplots_adjust(hspace=0.2, wspace=0.1)\n","          for i, (elem, ax) in enumerate(zip(mtrain, axes.flat)):\n","              image = tf.squeeze(elem['image'])\n","              # print(image)\n","              label = elem['label']\n","\n","              ax.imshow(image, cmap='gray')\n","              ax.text(0.7, -0.12, f'Digit = {label}', ha='right',\n","                      transform=ax.transAxes, color='black')\n","              ax.set_xticks([])\n","              ax.set_yticks([])\n","              # plt.show()\n","\n","        if 'corrupted' not in datasets_name:\n","            #\n","            print(' - Print one train set image:')\n","            for train_batch in datasets[datasets_name]['train'].take(1):\n","                image = train_batch[0].numpy()\n","            image = np.squeeze(image)\n","            plt.figure()\n","            plt.imshow(image, cmap='gray')\n","            plt.colorbar()\n","            plt.grid(False)\n","            plt.axis('off')\n","            plt.show();\n","\n","        #\n","        print(' - Print one test set image:')\n","        for test_batch in datasets[datasets_name]['test'].take(1):\n","            image = test_batch[0].numpy()\n","        image = np.squeeze(image)\n","        plt.figure()\n","        plt.imshow(image, cmap='gray')\n","        plt.colorbar()\n","        plt.grid(False)\n","        plt.axis('off')\n","        plt.show();\n","\n","        #\n","        print(' - Print one test set label:')\n","        for test_label in datasets[datasets_name]['test_label'].take(1):\n","            label = test_label[0]\n","        print(label.numpy())\n","        print('\\n')\n","\n","    return datasets"]},{"cell_type":"markdown","metadata":{"id":"Wt4eHh5pl04k"},"source":["## Sampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZbD9Bkuul1Fl"},"outputs":[],"source":["class Sampler_Z(tfkl.Layer):   \n","\n","    def call(self, inputs):\n","        mean, logvar = inputs\n","        # Reparameterize\n","        eps = tf.random.normal(shape=mean.shape, seed=0)\n","        z_sample = eps * tf.exp(logvar * .5) + mean\n","        #tf.print(tf.math.reduce_mean(eps))\n","        return z_sample\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Vtxx6SXPl-xy"},"source":["## Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6F3QUIUl-5C"},"outputs":[],"source":["# Encoder/Decoder layers 1 (for MNIST images)\n","class EncoderZ_1(tfkl.Layer):\n","\n","    def __init__(self, \n","                 z_dim, \n","                 n_filter_base, \n","                 seed, \n","                 dtype, \n","                 name='encoder', \n","                 **kwargs\n","                 ):\n","        super(EncoderZ_1, self).__init__(name=name, **kwargs)\n","        # Block-1\n","        self.conv_layer_1 = tfkl.Conv2D(\n","            filters=n_filter_base, \n","            kernel_size=3,\n","            strides=1, \n","            padding='same', \n","            name='conv_1',\n","            dtype=dtype\n","            )\n","\n","        self.batch_layer_1 = tfkl.BatchNormalization(\n","            name='bn_1', \n","            dtype=dtype\n","            )\n","        self.activation_layer_1 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_1', \n","            dtype=dtype\n","            )\n","        # Block-2\n","        self.conv_layer_2 = tfkl.Conv2D(\n","            filters=n_filter_base*2, \n","            kernel_size=3,\n","            strides=2, \n","            padding='same', \n","            name='conv_2', \n","            dtype=dtype\n","            )\n","        self.batch_layer_2 = tfkl.BatchNormalization(name='bn_2', dtype=dtype)\n","        self.activation_layer_2 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_2', \n","            dtype=dtype\n","            )\n","        # Block-3\n","        self.conv_layer_3 = tfkl.Conv2D(\n","            filters=n_filter_base*2, \n","            kernel_size=3,\n","            strides=2, \n","            padding='same', \n","            name='conv_3', \n","            dtype=dtype\n","            )\n","        self.batch_layer_3 = tfkl.BatchNormalization(name='bn_3', dtype=dtype)\n","        self.activation_layer_3 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_3', \n","            dtype=dtype\n","            )\n","        # Block-4\n","        self.conv_layer_4 = tfkl.Conv2D(\n","            filters=n_filter_base*2, \n","            kernel_size=3,\n","            strides=1, \n","            padding='same', \n","            name='conv_4', \n","            dtype=dtype\n","            )\n","        self.batch_layer_4 = tfkl.BatchNormalization(name='bn_4', dtype=dtype)\n","        self.activation_layer_4 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_4', \n","            dtype=dtype\n","            )\n","        # Final Block\n","        self.flatten_layer = Flatten()\n","        self.dense_mean = Dense(\n","            z_dim, \n","            activation=None, \n","            name='z_mean', \n","            dtype=dtype\n","            )\n","        self.dense_raw_stddev = Dense(\n","            z_dim, \n","            activation=None,\n","            name='z_raw_stddev', \n","            dtype=dtype\n","            )\n","        self.sampler_z = Sampler_Z()\n","\n","    # Functional\n","    def call(self, x_input):\n","        z = self.conv_layer_1(x_input)\n","        z = self.batch_layer_1(z)\n","        z = self.activation_layer_1(z)\n","        z = self.conv_layer_2(z)\n","        z = self.batch_layer_2(z)\n","        z = self.activation_layer_2(z)\n","        z = self.conv_layer_3(z)\n","        z = self.batch_layer_3(z)\n","        z = self.activation_layer_3(z)\n","        z = self.conv_layer_4(z)\n","        z = self.batch_layer_4(z)\n","        z = self.activation_layer_4(z)\n","        z = self.flatten_layer(z)\n","        mean = self.dense_mean(z)\n","        logvar = self.dense_raw_stddev(z)\n","        z_sample = self.sampler_z((mean, logvar))\n","        return z_sample, mean, logvar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pQzMpuEyCW8"},"outputs":[],"source":["# Encoder/Decoder layers 1 (for MNIST images)\n","# EncoderZ_2 is identical to EncoderZ_1 aside from the addition of an intermediate dense layer before the final output layer.\n","class EncoderZ_2(tfkl.Layer):\n","\n","    def __init__(self, \n","                 z_dim, \n","                 n_filter_base, \n","                 seed, \n","                 dtype, \n","                 name='encoder', \n","                 **kwargs\n","                 ):\n","        super(EncoderZ_2, self).__init__(name=name, **kwargs)\n","        # Block-1\n","        self.conv_layer_1 = tfkl.Conv2D(\n","            filters=n_filter_base, \n","            kernel_size=3,\n","            strides=1, \n","            padding='same', \n","            name='conv_1',\n","            dtype=dtype\n","            )\n","        self.batch_layer_1 = tfkl.BatchNormalization(\n","            name='bn_1', \n","            dtype=dtype\n","            )\n","        self.activation_layer_1 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_1', \n","            dtype=dtype\n","            )\n","        # Block-2\n","        self.conv_layer_2 = tfkl.Conv2D(\n","            filters=n_filter_base*2, \n","            kernel_size=3,\n","            strides=2, \n","            padding='same', \n","            name='conv_2', \n","            dtype=dtype\n","            )\n","        self.batch_layer_2 = tfkl.BatchNormalization(name='bn_2', dtype=dtype)\n","        self.activation_layer_2 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_2', \n","            dtype=dtype\n","            )\n","        # Block-3\n","        self.conv_layer_3 = tfkl.Conv2D(\n","            filters=n_filter_base*2, \n","            kernel_size=3,\n","            strides=2, \n","            padding='same', \n","            name='conv_3', \n","            dtype=dtype\n","            )\n","        self.batch_layer_3 = tfkl.BatchNormalization(name='bn_3', dtype=dtype)\n","        self.activation_layer_3 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_3', \n","            dtype=dtype\n","            )\n","        # Block-4\n","        self.conv_layer_4 = tfkl.Conv2D(\n","            filters=n_filter_base*2, \n","            kernel_size=3,\n","            strides=1, \n","            padding='same', \n","            name='conv_4', \n","            dtype=dtype\n","            )\n","        self.batch_layer_4 = tfkl.BatchNormalization(name='bn_4', dtype=dtype)\n","        self.activation_layer_4 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_4', \n","            dtype=dtype\n","            )\n","        # FC Block\n","        self.flatten_layer = Flatten()\n","        self.fc_layer_1 = Dense(128, activation='relu', name = 'fc_1', dtype=dtype)\n","        # Final Block\n","        self.dense_mean = Dense(\n","            z_dim, \n","            activation=None, \n","            name='z_mean', \n","            dtype=dtype\n","            )\n","        self.dense_raw_stddev = Dense(\n","            z_dim, \n","            activation=None,\n","            name='z_raw_stddev', \n","            dtype=dtype\n","            )\n","        self.sampler_z = Sampler_Z()\n","\n","    # Functional\n","    def call(self, x_input):\n","        z = self.conv_layer_1(x_input)\n","        z = self.batch_layer_1(z)\n","        z = self.activation_layer_1(z)\n","        z = self.conv_layer_2(z)\n","        z = self.batch_layer_2(z)\n","        z = self.activation_layer_2(z)\n","        z = self.conv_layer_3(z)\n","        z = self.batch_layer_3(z)\n","        z = self.activation_layer_3(z)\n","        z = self.conv_layer_4(z)\n","        z = self.batch_layer_4(z)\n","        z = self.activation_layer_4(z)\n","        z = self.flatten_layer(z)\n","        z = self.fc_layer_1(z)\n","        mean = self.dense_mean(z)\n","        logvar = self.dense_raw_stddev(z)\n","        z_sample = self.sampler_z((mean, logvar))\n","        return z_sample, mean, logvar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0iRuLWbIq1z"},"outputs":[],"source":["# Gaussian MLP as encoder - this architecture is taken from the student's paper \n","# and renamed to EncoderZ_3 for consistency, and re-written to better match the\n","# syntax of the other networks\n","class EncoderZ_3(tfkl.Layer):\n","    def __init__(self, \n","                 z_dim, \n","                 n_filter_base, \n","                 seed, \n","                 dtype, \n","                 name='encoder', \n","                 **kwargs\n","                 ):\n","        super(EncoderZ_3, self).__init__(name=name, **kwargs)\n","        # flatten because we are immediately starting with dense blocks\n","        self.flatten_layer = Flatten()\n","        # Block-1\n","        self.fc_layer_1 = Dense(int((500/32)*n_filter_base), activation='elu', name = 'fc_1', dtype=dtype)\n","        self.drop1 = Dropout(0.1)\n","        # Block-2\n","        self.fc_layer_2 = Dense(int((500/32)*n_filter_base), activation='tanh', name='fc_2', dtype=dtype)\n","        self.drop2 = Dropout(0.1)\n","        # Final Block\n","        self.dense_mean = Dense(\n","            z_dim, \n","            activation=None, \n","            name='z_mean', \n","            dtype=dtype\n","            )\n","        self.dense_raw_stddev = Dense(\n","            z_dim, \n","            activation=None,\n","            name='z_raw_stddev', \n","            dtype=dtype\n","            )\n","        self.sampler_z = Sampler_Z()\n","\n","    # Functional\n","    def call(self, x_input):\n","        z = self.flatten_layer(x_input)\n","        z = self.fc_layer_1(z)\n","        z = self.drop1(z)\n","        z = self.fc_layer_2(z)\n","        z = self.drop2(z)\n","        mean = self.dense_mean(z)\n","        logvar = self.dense_raw_stddev(z)\n","        z_sample = self.sampler_z((mean, logvar))\n","        return z_sample, mean, logvar"]},{"cell_type":"markdown","metadata":{"id":"Qxh11GiRmBqW"},"source":["## Decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-UE5_aamGVw"},"outputs":[],"source":["class DecoderX_1(tfkl.Layer):\n","\n","    def __init__(self, z_dim, n_filter_base, dtype, name='decoder', **kwargs):\n","        super(DecoderX_1, self).__init__(name=name, **kwargs)\n","        # For MNIST images\n","        self.dense_z_input = tfkl.Dense(\n","            units=7*7*n_filter_base*2,\n","            activation=tf.nn.relu, \n","            dtype=dtype\n","            )\n","        self.reshape_layer = tfkl.Reshape(target_shape=(7, 7, n_filter_base*2))\n","        # Block-1\n","        self.conv_transpose_layer_1 = Conv2DTranspose(\n","            filters=n_filter_base*2,\n","            kernel_size=3,\n","            strides=1, \n","            padding='same',\n","            name='conv_transpose_1', \n","            dtype=dtype\n","            )\n","        self.batch_layer_1 = tfkl.BatchNormalization(name='bn_1', dtype=dtype)\n","        self.activation_layer_1 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_1', \n","            dtype=dtype\n","            )\n","        # Block-2\n","        self.conv_transpose_layer_2 = Conv2DTranspose(\n","            filters=n_filter_base*2,\n","            kernel_size=3,\n","            strides=2, \n","            padding='same',\n","            name='conv_transpose_2', \n","            dtype=dtype\n","            )\n","        self.batch_layer_2 = tfkl.BatchNormalization(name='bn_2', dtype=dtype)\n","        self.activation_layer_2 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_2', \n","            dtype=dtype\n","            )\n","        # Block-3\n","        self.conv_transpose_layer_3 = Conv2DTranspose(\n","            filters=n_filter_base,\n","            kernel_size=3,\n","            strides=2, \n","            padding='same',\n","            name='conv_transpose_3', \n","            dtype=dtype\n","            )\n","        self.batch_layer_3 = tfkl.BatchNormalization(name='bn_3', dtype=dtype)\n","        self.activation_layer_3 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_3', \n","            dtype=dtype\n","            )\n","        # Block-4\n","        # Filters=1 for gray-scaled images\n","        self.conv_transpose_layer_4 = Conv2DTranspose(\n","            filters=1,\n","            kernel_size=3,\n","            strides=1, \n","            padding='same',\n","            name='conv_transpose_4',\n","            dtype=dtype\n","            )\n","\n","    # Functional\n","    def call(self, z):\n","        x_output = self.dense_z_input(z)\n","        x_output = self.reshape_layer(x_output)\n","        x_output = self.conv_transpose_layer_1(x_output)\n","        x_output = self.batch_layer_1(x_output)\n","        x_output = self.activation_layer_1(x_output)\n","        x_output = self.conv_transpose_layer_2(x_output)\n","        x_output = self.batch_layer_2(x_output)\n","        x_output = self.activation_layer_2(x_output)\n","        x_output = self.conv_transpose_layer_3(x_output)\n","        x_output = self.batch_layer_3(x_output)\n","        x_output = self.activation_layer_3(x_output)\n","        x_output = self.conv_transpose_layer_4(x_output)\n","        return x_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZGug8JQ0AbB"},"outputs":[],"source":["# Like Z_2 above, DecoderX_2 is identical to DecoderX_1 except it contains an additional dense layer between the latent space and the rest of the decoder network.\n","class DecoderX_2(tfkl.Layer):\n","\n","    def __init__(self, z_dim, n_filter_base, dtype, name='decoder', **kwargs):\n","        super(DecoderX_2, self).__init__(name=name, **kwargs)\n","        # Start with a dense latent layer\n","        self.fc_input_1 = Dense(128, activation='relu', name = 'fc_i1', dtype=dtype)\n","        # For MNIST images\n","        self.dense_z_input = tfkl.Dense(\n","            units=7*7*n_filter_base*2,\n","            activation=tf.nn.relu, \n","            dtype=dtype\n","            )\n","        self.reshape_layer = tfkl.Reshape(target_shape=(7, 7, n_filter_base*2))\n","        # Block-1\n","        self.conv_transpose_layer_1 = Conv2DTranspose(\n","            filters=n_filter_base*2,\n","            kernel_size=3,\n","            strides=1, \n","            padding='same',\n","            name='conv_transpose_1', \n","            dtype=dtype\n","            )\n","        self.batch_layer_1 = tfkl.BatchNormalization(name='bn_1', dtype=dtype)\n","        self.activation_layer_1 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_1', \n","            dtype=dtype\n","            )\n","        # Block-2\n","        self.conv_transpose_layer_2 = Conv2DTranspose(\n","            filters=n_filter_base*2,\n","            kernel_size=3,\n","            strides=2, \n","            padding='same',\n","            name='conv_transpose_2', \n","            dtype=dtype\n","            )\n","        self.batch_layer_2 = tfkl.BatchNormalization(name='bn_2', dtype=dtype)\n","        self.activation_layer_2 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_2', \n","            dtype=dtype\n","            )\n","        # Block-3\n","        self.conv_transpose_layer_3 = Conv2DTranspose(\n","            filters=n_filter_base,\n","            kernel_size=3,\n","            strides=2, \n","            padding='same',\n","            name='conv_transpose_3', \n","            dtype=dtype\n","            )\n","        self.batch_layer_3 = tfkl.BatchNormalization(name='bn_3', dtype=dtype)\n","        self.activation_layer_3 = tfkl.Activation(\n","            tf.nn.leaky_relu, \n","            name='lrelu_3', \n","            dtype=dtype\n","            )\n","        # Block-4\n","        # Filters=1 for gray-scaled images\n","        self.conv_transpose_layer_4 = Conv2DTranspose(\n","            filters=1,\n","            kernel_size=3,\n","            strides=1, \n","            padding='same',\n","            name='conv_transpose_4',\n","            dtype=dtype\n","            )\n","\n","    # Functional\n","    def call(self, z):\n","        x_output = self.fc_input_1(z)\n","        x_output = self.dense_z_input(x_output)\n","        x_output = self.reshape_layer(x_output)\n","        x_output = self.conv_transpose_layer_1(x_output)\n","        x_output = self.batch_layer_1(x_output)\n","        x_output = self.activation_layer_1(x_output)\n","        x_output = self.conv_transpose_layer_2(x_output)\n","        x_output = self.batch_layer_2(x_output)\n","        x_output = self.activation_layer_2(x_output)\n","        x_output = self.conv_transpose_layer_3(x_output)\n","        x_output = self.batch_layer_3(x_output)\n","        x_output = self.activation_layer_3(x_output)\n","        x_output = self.conv_transpose_layer_4(x_output)\n","        return x_output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtqWOEXp7Pr4"},"outputs":[],"source":["# DecoderX_3 is the Bernoulli MLP decoder adapted to the syntax styles of this notebook\n","class DecoderX_3(tfkl.Layer):\n","\n","    def __init__(self, z_dim, n_filter_base, dtype, name='decoder', **kwargs):\n","        super(DecoderX_3, self).__init__(name=name, **kwargs)\n","        # Start with a dense latent layer\n","        self.fc_input_1 = Dense(int((500/32)*n_filter_base), activation='tanh', name = 'fc_i1', dtype=dtype)\n","        self.drop1 = Dropout(0.1)\n","        # Block-2\n","        self.fc_layer_2 = Dense(int((500/32)*n_filter_base), activation='elu', name='fc_2', dtype=dtype)\n","        self.drop2 = Dropout(0.1)\n","        # output layer\n","        self.fc_layer_3 = Dense(28*28, activation='sigmoid', name='fc_3', dtype=dtype)\n","        self.reshape_layer = tfkl.Reshape(target_shape=(28, 28), dtype=dtype)\n","\n","    # Functional\n","    def call(self, z):\n","        x_output = self.fc_input_1(z)\n","        x_output = self.drop1(x_output)\n","        x_output = self.fc_layer_2(x_output)\n","        x_output = self.drop2(x_output)\n","        x_output = self.fc_layer_3(x_output)\n","        x_output = self.reshape_layer(x_output)\n","        return x_output"]},{"cell_type":"markdown","metadata":{"id":"EfocmOTImRFs"},"source":["## Base Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OU6_6VZ5mRMN"},"outputs":[],"source":["class VAEModel(tfk.Model):\n","    '''Convolutional variational autoencoder base model.\n","    '''\n","    \n","    def __init__(self, z_dim, n_filter_base, seed, dtype):\n","        super(VAEModel, self).__init__()\n","        self.encoder = EncoderZ_1(z_dim, n_filter_base, seed, dtype)\n","        self.decoder = DecoderX_1(z_dim, n_filter_base, dtype)\n","        return\n","    \n","    #@tf.function\n","    def sample(self, z_sample):\n","        x_recons_logits = self.decoder(z_sample)\n","        sample_images = tf.sigmoid(x_recons_logits)  # predictions\n","        return sample_images\n","    \n","    def call(self, x_input):\n","        z_sample, mean, logvar = self.encoder(x_input)\n","        x_recons_logits = self.decoder(z_sample)\n","        return x_recons_logits, z_sample, mean, logvar"]},{"cell_type":"markdown","metadata":{"id":"0-XA_4yGj2oG"},"source":["## GeneralizedMean"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXvC6BsXnqhV"},"outputs":[],"source":["class GeneralizedMean:\n","\n","    def __init__(self, ll_values, kl_values, kappa, z_dim):\n","        self.kappa = kappa\n","        self.z_dim = z_dim\n","        self.gmean_metrics = pd.Series()\n","        self.gmean_log_prob_values = {}\n","        self._save_generalized_mean_metrics('elbo', ll_values - kl_values)\n","        self._save_generalized_mean_metrics('recon', ll_values)\n","        self._save_generalized_mean_metrics('kldiv', -kl_values)\n","        return\n","    \n","    def _save_generalized_mean_metrics(self, key, log_prob_values):\n","        # Save the generalized means and metric values\n","        prob_values = tf.exp(log_prob_values)\n","        self.inv_n = 1 / len(prob_values)  # 1/n\n","        decisiveness = self._calculate_decisiveness(prob_values)\n","        accuracy = self._calculate_accuracy(prob_values)\n","        robustness = self._calculate_robustness(prob_values)\n","        r = kappa_to_r(float(self.kappa), dim=1)  # TODO USE Z-DIM?\n","        gen_mean = generalized_mean(prob_values, r)\n","        curr_metrics = pd.Series(\n","            [decisiveness, accuracy, robustness, gen_mean.numpy()],\n","            index=[\n","                f'{key}_decisiveness', f'{key}_accuracy', f'{key}_robustness',\n","                f'{key}_{round(r, 3)}_generalized_mean'\n","                ]\n","            )\n","        self.gmean_metrics = self.gmean_metrics.append(curr_metrics)\n","        self.gmean_log_prob_values[key] = log_prob_values\n","    \n","    def _calculate_decisiveness(self, values):\n","        # Decisiveness = Arithmetic mean\n","        result = tf.reduce_sum(values) * self.inv_n\n","        return result.numpy()\n","    \n","    def _calculate_accuracy(self, values):\n","        # Accuracy = Geometric mean\n","        result = tf.math.exp(tf.reduce_sum(self.inv_n * tf.math.log(values)))\n","        return result.numpy()\n","    \n","    def _calculate_robustness(self, values):\n","        # Robustness = -2/3 Mean\n","        result = tf.reduce_sum(self.inv_n * (values ** (-2/3))) ** (-3/2)\n","        return result.numpy()\n","    \n","    def get_metrics(self):\n","        return self.gmean_metrics\n","    \n","    def get_log_prob_values(self):\n","        return self.gmean_log_prob_values"]},{"cell_type":"markdown","metadata":{"id":"Vagw09Fqnq_O"},"source":["## Visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xK-iRbVMnrH9"},"outputs":[],"source":["class Visualize:\n","\n","    def __init__(self,\n","                 z_dim,\n","                 kappa,\n","                 sample_func, \n","                 z_sample,\n","                 sample,\n","                 sample_labels, \n","                 gmean_metrics,\n","                 gmean_log_prob_values, \n","                 display_path='.', \n","                 parameter_str=''\n","                 ):\n","        self.z_dim = z_dim\n","        self.kappa = kappa\n","        self.sample_func = sample_func\n","        self.z_sample = z_sample\n","        self.sample_labels = sample_labels\n","        self.gmean_metrics = gmean_metrics\n","        self.gmean_log_prob_values = gmean_log_prob_values\n","        self.display_path = display_path\n","        self.parameter_str = parameter_str\n","        if len(z_sample) <= 16:\n","            sample_images = sample_func(z_sample)  # predictions\n","            sample_images_orig = sample\n","        else:\n","            sample_images = sample_func(z_sample)[:16]  # first 16 predictions\n","            sample_images_orig = sample[:16]\n","\n","        self.sample_images = sample_images\n","        self.sample_images_orig = sample_images_orig\n","        self.digit_size = len(sample_images[0])\n","        return\n","    \n","    def display(self, show=True, **kwargs):\n","        self.display_generated_images(show=show, **kwargs)\n","        self.display_original_images(show=show, **kwargs)\n","        self.display_latent_space(show=show, **kwargs)\n","        #self.display_manifold(show=show, **kwargs)\n","        self.display_histogram(key='recon', show=show, **kwargs)\n","        self.display_histogram(key='kldiv', show=show, **kwargs)\n","        self.display_histogram(key='elbo', show=show, **kwargs)\n","        return\n","\n","\n","    def display_test(self, show=True, **kwargs):\n","        self.display_generated_images(show=show, **kwargs)\n","        self.display_original_images(show=show, **kwargs)\n","        self.display_latent_space(show=show, **kwargs)\n","        #self.display_manifold(show=show, **kwargs)\n","        \n","        self.display_histogram(key='recon', show=show, **kwargs)\n","        self.display_histogram(key='kldiv', show=show, **kwargs)\n","        self.display_histogram(key='elbo', show=show, **kwargs)\n","        \n","        return\n","    \n","    def display_generated_images(self, image_size=4, show=True, **kwargs):\n","\n","        fig = plt.figure(figsize=(image_size, image_size))\n","        for i in range(self.sample_images.shape[0]):\n","            plt.subplot(4, 4, i + 1)\n","            plt.imshow(self.sample_images[i, :, :, 0], cmap='gray')\n","            plt.axis('off')\n","            # tight_layout minimizes the overlap between 2 sub-plots\n","            plt.axis('Off')\n","            plt.savefig(f\"{self.display_path}/generated_images/\" + \\\n","                       f\"{self._picture_name('images', **kwargs)}\"\n","                    )\n","        if show:\n","            plt.show();\n","        return\n","\n","\n","    def display_original_images(self, image_size=4, show=True, **kwargs):\n","\n","        fig = plt.figure(figsize=(image_size, image_size))\n","        for i in range(self.sample_images_orig.shape[0]):\n","            plt.subplot(4, 4, i + 1)\n","            plt.imshow(self.sample_images_orig[i, :, :, 0], cmap='gray')\n","            plt.axis('off')\n","            # tight_layout minimizes the overlap between 2 sub-plots\n","            plt.axis('Off')\n","            plt.savefig(f\"{self.display_path}/generated_images/\" + \\\n","                       f\"{self._picture_name('original_images', **kwargs)}\"\n","                    )\n","        if show:\n","            plt.show();\n","        return\n","    \n","    def display_latent_space(self, image_size=4, show=True, **kwargs):\n","        # display a 2D plot of the digit classes in the latent space\n","        \n","        colors = ['pink', 'red', 'orange', 'yellow', 'green', \n","        'blue', 'purple', 'brown', 'gray', 'black']\n","        plt.figure(figsize=(image_size, image_size))\n","        z_dim = self.z_dim\n","        if z_dim==2:\n","            plt.scatter(\n","                self.z_sample[:, 0], \n","                self.z_sample[:, 1], \n","                c=self.sample_labels,\n","                cmap=matplotlib.colors.ListedColormap(colors)\n","                )\n","            plt.xlabel('z[0]')\n","            plt.ylabel('z[1]')\n","        \n","        else:\n","            encoded_imgs_embedded = TSNE(n_components=2).fit_transform(\n","                self.z_sample\n","                )\n","            plt.scatter(\n","                encoded_imgs_embedded[:,0],\n","                encoded_imgs_embedded[:,1],\n","                c=self.sample_labels,\n","                cmap=matplotlib.colors.ListedColormap(colors)\n","                )\n","            plt.xlabel(\"t-SNE 1st dimension\")\n","            plt.ylabel(\"t-SNE 2nd dimension\")\n","              \n","            plt.colorbar()\n","            plt.savefig(f\"{self.display_path}/latent_spaces/\" + \\\n","                        f\"{self._picture_name('latent', **kwargs)}\"\n","                        )\n","        if show:\n","            plt.show();\n","        return\n","    \n","    def display_manifold(self, n=20, image_size=10, show=True, **kwargs):\n","        \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n","        norm = tfp.distributions.Normal(0, 1)\n","        grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n","        grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n","        image_width = self.digit_size*n\n","        image_height = image_width\n","        image = np.zeros((image_height, image_width))\n","        for i, yi in enumerate(grid_x):\n","            for j, xi in enumerate(grid_y):\n","                z = np.array([[xi, yi]])\n","                x_decoded = self.sample_func(z)\n","                digit = tf.reshape(\n","                    x_decoded[0], \n","                    (self.digit_size, self.digit_size)\n","                    )\n","                image[i*self.digit_size: (i+1)*self.digit_size,\n","                      j*self.digit_size: (j+1)*self.digit_size] = digit.numpy()\n","        plt.figure(figsize=(image_size, image_size))\n","        plt.imshow(image, cmap='Greys_r')\n","        plt.axis('Off')\n","        plt.savefig(f\"{self.display_path}/manifolds/\" + \\\n","        f\"{self._picture_name('mani', **kwargs)}\"\n","        )\n","        if show:\n","            plt.show();\n","        return\n","\n","    def display_histogram(self, key, image_size=(24, 10), show=True, **kwargs):\n","        # Get histogram title and x-axis\n","        if key == 'recon':\n","            # xlabel = 'Probability of reconstructed image equivalent to original \n","            # one.'\n","            xlabel = 'Reconstruction Probability'\n","            xticks = [\n","                1e-240, 1e-210, 1e-180, 1e-150, 1e-120, 1e-90, 1e-60, 1e-30, 1\n","                ]\n","        elif key == 'kldiv':\n","            xlabel = 'Divergence Probability'\n","            xticks = [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n","        else:\n","            xlabel = 'ELBO Probability'\n","            xticks = [\n","                1e-240, 1e-210, 1e-180, 1e-150, 1e-120, 1e-90, 1e-60, 1e-30, 1\n","                ]\n","        # Retrieve the generalized means and metric values\n","        decisiveness = self.gmean_metrics[f'{key}_decisiveness']\n","        accuracy = self.gmean_metrics[f'{key}_accuracy']\n","        robustness = self.gmean_metrics[f'{key}_robustness']\n","        r = kappa_to_r(float(self.kappa), dim=1)  # TODO USE Z-DIM?\n","        r = round(r, 3)\n","        r_gen_mean = self.gmean_metrics[f'{key}_{r}_generalized_mean']\n","        log_prob_values = self.gmean_log_prob_values[key]\n","        # The histogram of the data\n","        fig, ax = plt.subplots(figsize=(24, 10))\n","        xtick_axis = [math.log(xtick) for xtick in xticks]\n","        xtick_labels = [str(xtick) for xtick in xticks]\n","        ax.set_xticks(xtick_axis)\n","        ax.set_xticklabels(xtick_labels)\n","        if kwargs['epoch'] != '':\n","          plt.title(\n","              f\"{xlabel} Histogram for VAE (Epoch={kwargs['epoch']})\", \n","              fontdict = {'fontsize' : 40, 'weight': 'bold'}\n","              )\n","        else:\n","          plt.title(\n","              f\"{xlabel} Histogram for VAE\", \n","              fontdict = {'fontsize' : 40, 'weight': 'bold'}\n","              )\n","        plt.xlabel(xlabel, fontdict = {'fontsize' : 40, 'weight': 'bold'})\n","        plt.ylabel(\n","            \"Frequency in logscale\", \n","            fontdict = {'fontsize' : 40, 'weight': 'bold'}\n","            )\n","        # Plot in logscale, so convert the metric as logs as well\n","        log_dec = np.log(decisiveness)\n","        log_acc = np.log(accuracy)\n","        log_rob = np.log(robustness)\n","        log_r_gen_mean = np.log(r_gen_mean)\n","        dec_txt = f'{decisiveness:0.2e}'\n","        acc_txt = f'{accuracy:0.2e}'\n","        rob_txt = f'{robustness:0.2e}'\n","        gen_mean_txt = f'{r_gen_mean:0.2e}'\n","        plt.axvline(log_dec, color='g', linestyle='dashed', linewidth=2)\n","        plt.text(log_dec, 10*12, dec_txt, color='g', size='large', weight='bold')\n","        plt.axvline(log_acc, color='b', linestyle='dashed', linewidth=2)\n","        plt.text(log_acc, 10*12, acc_txt, color='b', size='large', weight='bold')\n","        plt.axvline(log_rob, color='r', linestyle='dashed', linewidth=2)\n","        plt.text(log_rob, 10*12, rob_txt, color='r', size='large', weight='bold')\n","        if float(self.kappa) != 0:\n","          plt.axvline(log_r_gen_mean, color='k', linestyle='dashed', linewidth=2)\n","          plt.text(log_r_gen_mean, 10*12, gen_mean_txt, color='k', size='large', weight='bold')\n","        plt.hist(\n","            log_prob_values, \n","            log=True, \n","            bins=100, \n","            facecolor='white', \n","            edgecolor='black'\n","            )\n","        plt.savefig(\n","            f\"{self.display_path}/histograms/\" + \\\n","            f\"{self._picture_name(f'hist_{key}', **kwargs)}\"\n","            )\n","        if show:\n","            plt.show();\n","        return\n","    \n","    #Changed 8_26_2021 \"+ parameter_str\"\n","    def _picture_name(self, display_type, **kwargs):\n","        name = display_type + self.parameter_str\n","        for key, value in kwargs.items():\n","            name += f'_{key}{str(value)}'\n","        return f'{name}.png'"]},{"cell_type":"markdown","metadata":{"id":"_8Lfzn99ntgB"},"source":["## VAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aWU3DBsYMEYP"},"outputs":[],"source":["class VAE:\n","    \"\"\"Variational Autoencoder wrapper.\"\"\"\n","    def __init__(self, z_dim, n_filter_base=32, learning_rate=0.0005,\n","                 beta=1., p_std=1., seed=0, loss_coupling = 0.0,\n","                 analytic_kl=False, dtype='float32', display_path='.', \n","                 checkpoint_path='.',\n","                 ):\n","      self.optimizer = tfk.optimizers.Adam(learning_rate)\n","      self.model = VAEModel(z_dim, n_filter_base, seed, dtype)\n","      metrics_col = ['epoch', 'train_neg_elbo', 'train_recon_loss', 'train_coupled_div'] + \\\n","                    [f'{x}_{y}' for x in ['elbo', 'recon', 'coupled_div'] \n","                                for y in [\n","                                        'decisiveness', \n","                                        'accuracy', \n","                                        'robustness'\n","                                        ]\n","                    ]\n","      self.metrics_df = pd.DataFrame(columns=metrics_col)\n","      self.val_metrics_df = pd.DataFrame(columns=[\n","                                                  'val_neg_elbo', \n","                                                  'val_recon_loss', \n","                                                  'val_coupled_div'\n","                                                  ]\n","                                          )\n","      self.beta = beta\n","      self.p_std = p_std\n","      self.analytic_kl = analytic_kl\n","      self.display_path = display_path\n","      self._set_random_seeds(seed)\n","      self.loss_coupling = loss_coupling\n","      self.dtype=dtype\n","      self.z_dim = z_dim\n","\n","      print('Model initialized')\n","      return\n","\n","    def train(\n","            self, \n","            train_dataset, \n","            val_dataset, \n","            val_label,\n","            n_epoch=10, \n","            n_epoch_display=10, \n","            model_path = '.',\n","            show_display=False,\n","            early_stop=3\n","            ):\n","      epochs_since_last_improvement = 0\n","        \n","      print('Starting training')\n","      # declare an arbitrarly large loss for initialization\n","      best_val_score = float(\"inf\") \n","        \n","      # Pick a sample of the val set for generating output images\n","      for val_batch, val_batch_label in zip(\n","              val_dataset.take(1),\n","              val_label.take(1)\n","              ):\n","        val_sample = val_batch\n","        val_sample_label = val_batch_label\n","        \n","      for epoch in range(1, n_epoch + 1):\n","        if epochs_since_last_improvement < (early_stop + 1):\n","          # Training loop\n","          # Create empty lists to hold the training losses\n","          loss_lst, neg_ll_lst, kl_div_lst, ll_values_lst, kl_values_lst = [], [], [], [], []\n","          start_time = time.time()\n","          for train_x in tqdm(train_dataset):\n","            if epoch == 1: print(f'(Min, Max): {tf.math.reduce_min(train_x).numpy(), tf.math.reduce_max(train_x).numpy()}')  # TODO REMOVE\n","            loss, neg_ll, kl_div, ll_values, kl_values = self.train_step(train_x)\n","            loss_lst.append(loss)\n","            neg_ll_lst.append(neg_ll)\n","            kl_div_lst.append(kl_div)\n","            ll_values_lst.append(ll_values)\n","            kl_values_lst.append(kl_values)\n","          end_time = time.time()\n","          # Concatenate all the loss metric components into their own tensors.\n","          loss = tf.reduce_mean(tf.concat(loss_lst, axis=0))\n","          neg_ll = tf.reduce_mean(tf.concat(neg_ll_lst, axis=0))\n","          kl_div = tf.reduce_mean(tf.concat(kl_div_lst, axis=0))\n","          ll_values = tf.concat(ll_values_lst, axis=0)\n","          kl_values = tf.concat(kl_values_lst, axis=0)\n","\n","          # Get Validation Metrics\n","          # Only has one iteration, so not sure why loop is needed?\n","          for val_x in val_dataset:\n","            val_loss, val_neg_ll, val_kl_div, val_ll_values, val_kl_values = self.compute_loss(val_x, loss_coupling=self.loss_coupling)\n","\n","          display.clear_output(wait=False)\n","          print(\n","              f\"Epoch: {epoch}, Train set Loss: {loss},\\n \" + \\\n","              f\"Train set Recon: {neg_ll}, Train set KL: {kl_div}, \\n\" + \\\n","              f\"Val set Loss: {val_loss},\\n \" + \\\n","              f\"Val set Recon: {val_neg_ll}, Val set KL: {val_kl_div}, \\n\" + \\\n","              f\"time elapse for current epoch: {end_time - start_time}\"\n","              )\n","              \n","          if val_loss < best_val_score:\n","            best_val_score = val_loss\n","            print('Saving model checkpoint at epoch ' + str(epoch))\n","            print(model_path)\n","            self.model.save_weights(str(model_path / 'cp.ckpt'))\n","            epochs_since_last_improvement = 0\n","          else:\n","            epochs_since_last_improvement += 1\n","            \n","          # Generalized Mean\n","          gmean = GeneralizedMean(ll_values, kl_values, self.loss_coupling, self.z_dim)\n","\n","          # Visualize / Display\n","          display_list = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n","          if epoch in display_list:\n","            z_sample, _, _ = self.model.encoder(val_sample)\n","            viz = Visualize(self.z_dim,\n","                            self.loss_coupling,\n","                            self.model.sample,\n","                            z_sample,\n","                            val_sample,\n","                            val_sample_label,\n","                            gmean.get_metrics(),\n","                            gmean.get_log_prob_values(),\n","                            self.display_path\n","                            )\n","            viz.display(show=show_display,\n","                        cd='X',\n","                        cl='X',\n","                        epoch=epoch\n","                        )\n","          metrics_row = [\n","              int(epoch), loss.numpy(), neg_ll.numpy(), kl_div.numpy()\n","              ]\n","          metrics_row = pd.Series(\n","              metrics_row, \n","              index=self.metrics_df.columns[:4]\n","              )\n","          metrics_row = metrics_row.append(gmean.get_metrics())\n","          val_metrics_row = pd.Series(\n","              [\n","              val_loss.numpy(), val_neg_ll.numpy(), val_kl_div.numpy()\n","              ],\n","              index=self.val_metrics_df.columns\n","          )\n","          self.metrics_df = self.metrics_df.append(\n","              metrics_row, \n","              ignore_index=True\n","              )\n","          self.val_metrics_df = self.val_metrics_df.append(\n","              val_metrics_row,\n","              ignore_index=True\n","              )\n","        else:\n","          print(f'{epochs_since_last_improvement} epochs since last improvement. Stopping Training.')\n","          break\n","      return\n","\n","\n","    def test(\n","            self, \n","            test_corrupted,\n","            test_clean,\n","            test_label,\n","            save_path,\n","            show_display=False\n","            ):\n","      \n","      print('Starting testing')\n","\n","      # Pick a sample of the corrupted test set for generating output images\n","      for test_batch, test_batch_label in zip(\n","              test_corrupted.take(1),\n","              test_label.take(1)\n","              ):\n","        test_sample = test_batch\n","        test_sample_label = test_batch_label\n","        \n","      # Testing loop\n","      # Create empty lists to hold the training losses\n","      loss_lst, neg_ll_lst, kl_div_lst, ll_values_lst, kl_values_lst = [], [], [], [], []\n","\n","      i = 0  # TODO REMOVE\n","      for test_x in tqdm(zip(test_corrupted, test_clean)):\n","        if i == 0: print(f'Corrupted (Min, Max): {tf.math.reduce_min(test_x[0]).numpy(), tf.math.reduce_max(test_x[0]).numpy()}')\n","        if i == 0: print(f'Cleaned (Min, Max): {tf.math.reduce_min(test_x[1]).numpy(), tf.reduce_max(test_x[1]).numpy()}')\n","        loss, neg_ll, kl_div, ll_values, kl_values = self.compute_loss_test(test_x[0], test_x[1])\n","        loss_lst.append(loss)\n","        neg_ll_lst.append(neg_ll)\n","        kl_div_lst.append(kl_div)\n","        ll_values_lst.append(ll_values)\n","        kl_values_lst.append(kl_values)\n","        i += 1  # TODO REMOVE\n","\n","      # Concatenate all the loss metric components into their own tensors.\n","      loss = tf.reduce_mean(tf.concat(loss_lst, axis=0))\n","      neg_ll = tf.reduce_mean(tf.concat(neg_ll_lst, axis=0))\n","      kl_div = tf.reduce_mean(tf.concat(kl_div_lst, axis=0))\n","      ll_values = tf.concat(ll_values_lst, axis=0)\n","      kl_values = tf.concat(kl_values_lst, axis=0)\n","\n","      display.clear_output(wait=False)\n","      print(\n","          f\"Test set Loss: {loss}, \" + \\\n","          f\"Test set Recon: {neg_ll}, Test set KL: {kl_div}\"\n","          )\n","\n","      # Generalized Mean\n","      gmean = GeneralizedMean(ll_values, kl_values, self.loss_coupling, self.z_dim)\n","\n","      z_sample, _, _ = self.model.encoder(test_sample)\n","      viz = Visualize(self.z_dim,\n","                      self.loss_coupling,\n","                      self.model.sample,\n","                      z_sample,\n","                      test_sample,\n","                      test_sample_label,\n","                      gmean.get_metrics(),\n","                      gmean.get_log_prob_values(),\n","                      save_path\n","                      )\n","      viz.display_test(show=show_display,\n","                       cd='X',\n","                       cl='X',\n","                       epoch=''\n","                       )\n","\n","      metrics_row = [\n","          loss.numpy(), neg_ll.numpy(), kl_div.numpy()\n","          ]\n","      metrics_row = pd.Series(\n","          metrics_row, \n","          index=['neg_elbo', 'recon_loss', 'coupled_div']\n","          ) \n","\n","      metrics_row = metrics_row.append(gmean.get_metrics())\n","      \n","      return metrics_row\n","\n","\n","    #@tf.function\n","    def train_step(self, x_true, return_loss_components=True):\n","      \"\"\"Executes one training step and returns the loss.\n","\n","      This function computes the loss and gradients, and uses the latter to\n","      update the model's parameters.\n","      \"\"\"\n","      with tf.GradientTape() as tape:\n","          loss, neg_ll, kl_div, ll_values, kl_values = self.compute_loss(\n","              x_true, loss_only=False, loss_coupling=self.loss_coupling\n","              )\n","      gradients = tape.gradient(loss, self.model.trainable_variables)\n","      self.optimizer.apply_gradients(\n","          zip(gradients, self.model.trainable_variables)\n","          )\n","      if return_loss_components:\n","        return loss, neg_ll, kl_div, ll_values, kl_values\n","      return\n","\n","    def log_normal_pdf(self, sample, mean, logvar, raxis=1):\n","      log2pi = tf.math.log(2. * np.pi)\n","      return tf.reduce_sum(\n","          -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n","          axis=raxis)\n","    \n","    def coupled_kl_divergence_vs_standard_mvn_analytical(self, \n","                                                         loc, \n","                                                         scale, \n","                                                         kappa\n","                                                         ):\n","      \"\"\"\n","      This function calculates the coupled divergence between the input \n","      distribution and the multivariate Gaussian centered at the origin\n","      whose covariance matrix is the identity matrix.\n","      Parameters\n","      ----------\n","      loc : tf.Tensor\n","      scale : tf.Tensor\n","      kappa : float\n","          The coupling of the divergence function.\n","      Returns\n","      -------\n","      coupted_divergence : tf.Tensor\n","          The coupled divergence between an input distribution and the \n","          standard multivariate Gaussian.\n","      \"\"\"\n","\n","      loc_q = loc\n","      scale_q = diag_part(scale)\n","      dim = loc.shape[-1]\n","\n","      d1 = 1 + dim*kappa + 2*kappa  \n","\n","      coupled_div_t1 = add(\n","          multiply(kappa, nsc_tf.math.function.coupled_logarithm(\n","              2*pi, kappa=kappa, dim=dim)), 1\n","          )\n","      coupled_div_t1 = multiply(\n","          coupled_div_t1, \n","          sqrt(\n","              divide(d1, (subtract(d1, multiply(2*kappa, square(scale_q)))))\n","              )\n","          )\n","\n","      coupled_div_t1 *= exp(\n","          divide(multiply(multiply(square(loc_q), d1), kappa), \n","                multiply((1 + dim*kappa), \n","                          (subtract(d1, 2*kappa*square(scale_q)))))\n","          )\n","      coupled_div_t1 = reduce_prod(coupled_div_t1, axis=1)\n","\n","      coupled_div_t2 = multiply(kappa, \n","                                nsc_tf.math.function.coupled_logarithm(\n","                                    multiply(2*pi, square(scale_q)), \n","                                    kappa=kappa, \n","                                    dim=dim)\n","                                )\n","      coupled_div_t2 = add(coupled_div_t2, 1)\n","      coupled_div_t2 *= sqrt(d1 / (1 + dim*kappa))\n","      coupled_div_t2 = reduce_prod(coupled_div_t2, axis=1)\n","\n","      coupled_divergence = subtract(coupled_div_t1, coupled_div_t2) \n","      coupled_divergence = divide(coupled_divergence, 2*kappa)\n","\n","      return coupled_divergence\n","\n","\n","    def _compute_loss_orig(self, x, y, mean_vector, std_vector):\n","        coupling_loss = self.coupling_loss\n","        d = self.z_dim\n","        # Nonextensive statistical mechanics\n","        # q_tsallis = 1 + 2*coupling_loss/(1 + d*coupling_loss)\n","        d1 = 1 + d*coupling_loss + 2*coupling_loss\n","        #### Latent loss and image loss for Coupled VAE.\n","        marginal_likelihood = tf.reduce_sum(x * tf.subtract(tf.pow(y, (2*coupling_loss) / (1 + coupling_loss)), 1) / (2*coupling_loss) + (1 - x)\n","                                            * tf.subtract(tf.pow(1 - y, (2 * coupling_loss) / (1 + coupling_loss)), 1) / (coupling_loss*2), 1)\n","        kl_d1 = tf.reduce_prod(tf.pow(2 * tf.constant(math.pi), coupling_loss/(1 + d*coupling_loss)) * tf.sqrt(d1 / (d1 - 2*coupling_loss*tf.square(std_vector)))\n","                            * tf.exp(tf.square(mean_vector)*d1*coupling_loss / (1 + d*coupling_loss) / (d1 - 2*coupling_loss*tf.square(std_vector))), 1)\n","        kl_d2 = tf.reduce_prod(tf.pow(2 * tf.constant(math.pi)*tf.square(std_vector),\n","                                    coupling_loss / (1 + coupling_loss*d)) * tf.sqrt(d1 / (1 + d*coupling_loss)), 1)\n","        kl_divergence = (kl_d1 - kl_d2) / coupling_loss / 2\n","        kl_divergence = tf.reduce_mean(kl_divergence)\n","        marginal_likelihood = tf.reduce_mean(marginal_likelihood)\n","        # ELBO method\n","        elbo = marginal_likelihood - kl_divergence\n","        vae_loss = -elbo\n","        return vae_loss\n","    \n","\n","    def _coupled_likelihood_alt(self, x, y, coupling_loss):\n","\n","        marginal_likelihood = tf.reduce_sum(x * tf.subtract(tf.pow(y, (2*coupling_loss) / (1 + coupling_loss)), 1) / (2*coupling_loss) + (1 - x)\n","                                            * tf.subtract(tf.pow(1 - y, (2 * coupling_loss) / (1 + coupling_loss)), 1) / (coupling_loss*2), 1)\n","\n","        return marginal_likelihood\n","\n","\n","    def _coupled_likelihood(self, x, y, coupling_loss):\n","\n","        marginal_likelihood = x\n","        marginal_likelihood *= tf.subtract(tf.pow(y, (2*coupling_loss) / (1 + coupling_loss)), 1)\n","        marginal_likelihood /= (2*coupling_loss) + (1 - x)\n","        marginal_likelihood *= tf.subtract(tf.pow(1 - y, (2 * coupling_loss) / (1 + coupling_loss)), 1)\n","        marginal_likelihood /= (coupling_loss*2)\n","        marginal_likelihood = tf.reduce_sum(marginal_likelihood, 1)\n","\n","        return marginal_likelihood\n","    \n","\n","    def _coupled_div_orig(self, mean_vector, std_vector, coupling_loss):\n","        d = self.z_dim\n","        # Nonextensive statistical mechanics\n","        # q_tsallis = 1 + 2*coupling_loss/(1 + d*coupling_loss)\n","        d1 = 1 + d*coupling_loss + 2*coupling_loss\n","\n","        kl_d1 = tf.reduce_prod(tf.pow(2 * tf.constant(math.pi, dtype=tf.float64), coupling_loss/(1 + d*coupling_loss)) * tf.sqrt(d1 / (d1 - 2*coupling_loss*tf.square(std_vector)))\n","                            * tf.exp(tf.square(mean_vector)*d1*coupling_loss / (1 + d*coupling_loss) / (d1 - 2*coupling_loss*tf.square(std_vector))), 1)\n","        kl_d2 = tf.reduce_prod(tf.pow(2 * tf.constant(math.pi, dtype=tf.float64)*tf.square(std_vector),\n","                                    coupling_loss / (1 + coupling_loss*d)) * tf.sqrt(d1 / (1 + d*coupling_loss)), 1)\n","        kl_divergence = (kl_d1 - kl_d2) / coupling_loss / 2\n","        return kl_divergence\n","\n","\n","    def _coupled_div_alt(self, mean_vector, std_vector, coupling_loss):\n","\n","        d = self.z_dim\n","\n","        # Nonextensive statistical mechanics\n","        # q_tsallis = 1 + 2*coupling_loss/(1 + d*coupling_loss)\n","        d1 = 1 + d*coupling_loss + 2*coupling_loss\n","\n","        kl_d1 = tf.cast(coupling_loss/(1 + d*coupling_loss), tf.float64)\n","        kl_d1 *= tf.math.log(2 * tf.constant(math.pi, dtype=tf.float64))\n","        kl_d1 += tf.math.log(tf.sqrt(d1 / (d1 - 2*coupling_loss*tf.square(std_vector))))\n","        kl_d1 += tf.square(mean_vector)*d1*coupling_loss / (1 + d*coupling_loss) / (d1 - 2*coupling_loss*tf.square(std_vector))\n","        kl_d1 = tf.reduce_sum(kl_d1, 1)\n","        kl_d1 = tf.exp(kl_d1)\n","\n","        kl_d2 = coupling_loss / (1 + coupling_loss*d)\n","        kl_d2 *= tf.math.log(2 * tf.constant(math.pi, dtype=tf.float64)*tf.square(std_vector))\n","        kl_d2 += tf.math.log(tf.sqrt(d1 / (1 + d*coupling_loss)))\n","        kl_d2 = tf.reduce_sum(kl_d2, 1)\n","\n","        kl_d2 = tf.exp(kl_d2)\n","\n","        kl_divergence = (kl_d1 - kl_d2) / coupling_loss / 2\n","\n","        return kl_divergence\n","\n","\n","    def _coupled_div(self, mean_vector, std_vector, coupling_loss):\n","\n","        d = self.z_dim\n","\n","        # Nonextensive statistical mechanics\n","        # q_tsallis = 1 + 2*coupling_loss/(1 + d*coupling_loss)\n","        d1 = 1 + d*coupling_loss + 2*coupling_loss  # What is d1?\n","\n","        kl_d1 = tf.pow(2 * tf.constant(math.pi, dtype=tf.float64), tf.cast(coupling_loss/(1 + d*coupling_loss), tf.float64))\n","        kl_d1 *= tf.sqrt(d1 / (d1 - 2*coupling_loss*tf.square(std_vector)))\n","        kl_d1 *= tf.exp(\n","                tf.square(mean_vector)*d1*coupling_loss / (1 + d*coupling_loss) / (d1 - 2*coupling_loss*tf.square(std_vector))\n","                )\n","        kl_d1 = tf.reduce_prod(kl_d1, 1)\n","\n","        kl_d2 = kl_d2 = tf.pow(\n","            2 * tf.constant(math.pi, dtype=tf.float64)*tf.square(std_vector),\n","            coupling_loss / (1 + coupling_loss*d)\n","            )\n","        kl_d2 *= tf.sqrt(d1 / (1 + d*coupling_loss))\n","        kl_d2 = tf.reduce_prod(kl_d2, 1)\n","\n","        kl_divergence = (kl_d1 - kl_d2) / coupling_loss / 2\n","\n","        return kl_divergence\n","\n","\n","    def compute_loss(self, x_true, loss_only=False, loss_coupling=0.0):\n","      x_recons_logits, z_sample, mean, logvar = self.model(x_true)\n","\n","\n","      if loss_coupling == 0.0:\n","        # Sigmoid Cross Entropy Loss\n","        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(\n","            logits=x_recons_logits,\n","            labels=x_true\n","            )\n","        # Negative Log-Likelihood\n","        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3]) # log-likelihood\n","        neg_ll = -logpx_z  # negative log-likelihood\n","        # KL-Divergence\n","        if self.analytic_kl == True:\n","          sd = tf.math.log(1 + tf.math.exp(logvar))\n","          kl_div = -0.5 * tf.math.reduce_sum(\n","              1 + tf.math.log(tf.math.square(sd)) - \\\n","              tf.math.square(mean) - \\\n","              tf.math.square(sd),\n","              axis=1\n","              )\n","        else:\n","          logpz = self.log_normal_pdf(z_sample, 0., self.p_std)\n","          logqz_x = self.log_normal_pdf(z_sample, mean, logvar)\n","          kl_div = logqz_x - logpz\n","        # ELBO\n","        neg_ll_mean = tf.math.reduce_mean(neg_ll)\n","        kl_div_mean = tf.math.reduce_mean(kl_div)\n","        # elbo = -self.beta*kl_div_mean + neg_ll_mean\n","        loss = neg_ll_mean + self.beta*kl_div_mean\n","\n","      else:\n","        ##NSC ELBO\n","        \n","        #Conversion from logits to probs\n","        p = x_true\n","        q = tf.math.sigmoid(x_recons_logits)\n","\n","        #Calculation of binary log_loss\n","        cross_ent_2 = p*nsc_tf.math.function.coupled_logarithm(\n","            q, \n","            kappa=self.loss_coupling) + (\n","                1-p\n","                )*nsc_tf.math.function.coupled_logarithm(\n","                    1-q, \n","                    kappa=self.loss_coupling\n","                    )\n","\n","        logpx_z= tf.reduce_sum(cross_ent_2, axis=[1, 2, 3])\n","        neg_ll = -logpx_z\n","\n","        '''\n","        p = x_true\n","        q = tf.math.sigmoid(x_recons_logits)\n","        logpx_z = self._coupled_likelihood(q, p, self.loss_coupling)\n","        neg_ll = -logpx_z\n","        '''\n","        kl_div = self._coupled_div_orig(mean, tf.exp(logvar/2), self.loss_coupling)\n","        '''\n","        kl_div = self.coupled_kl_divergence_vs_standard_mvn_analytical(\n","            loc=mean, \n","            scale=tf.linalg.diag(tf.exp(logvar/2)), \n","            kappa=self.loss_coupling\n","            )\n","        '''\n","        neg_ll_mean = tf.math.reduce_mean(neg_ll)\n","        kl_div_mean = tf.math.reduce_mean(kl_div)\n","\n","        loss = neg_ll_mean + self.beta*kl_div_mean\n","\n","\n","      if loss_only:\n","          return loss\n","      return loss, neg_ll_mean, kl_div_mean, \\\n","              tf.cast(logpx_z, tf.float64), tf.cast(kl_div, tf.float64)\n","\n","\n","    def compute_loss_test(self, x_corrupt, x_true, loss_only=False):\n","      x_recons_logits, z_sample, mean, logvar = self.model(x_corrupt)\n","\n","      # Sigmoid Cross Entropy Loss\n","      cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(\n","          logits=x_recons_logits,\n","          labels=x_true\n","          )\n","      # Negative Log-Likelihood\n","      logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3]) # log-likelihood\n","      neg_ll = -logpx_z  # negative log-likelihood\n","      # KL-Divergence\n","      if self.analytic_kl == True:\n","        sd = tf.math.log(1 + tf.math.exp(logvar))\n","        kl_div = -0.5 * tf.math.reduce_sum(\n","            1 + tf.math.log(tf.math.square(sd)) - \\\n","            tf.math.square(mean) - \\\n","            tf.math.square(sd),\n","            axis=1\n","            )\n","      else:\n","        logpz = self.log_normal_pdf(z_sample, 0., self.p_std)\n","        logqz_x = self.log_normal_pdf(z_sample, mean, logvar)\n","        kl_div = logqz_x - logpz\n","      # ELBO\n","      neg_ll_mean = tf.math.reduce_mean(neg_ll)\n","      kl_div_mean = tf.math.reduce_mean(kl_div)\n","      # elbo = -self.beta*kl_div_mean + neg_ll_mean\n","      loss = neg_ll_mean + self.beta*kl_div_mean\n","\n","      if loss_only:\n","          return loss\n","      return loss, neg_ll_mean, kl_div_mean, \\\n","              tf.cast(logpx_z, tf.float64), tf.cast(kl_div, tf.float64)\n","\n","    def _set_random_seeds(self, seed):\n","      tf.random.set_seed(seed)\n","      np.random.seed(seed)\n","      random.seed(seed)\n","      return\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YASkxNpHnOK1"},"source":["## Plotting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdkIX04_nOSE"},"outputs":[],"source":["def plot_latent_images(model, n, digit_size=28):\n","    \"\"\"Plots n x n digit images decoded from the latent space.\"\"\"\n","    norm = tfp.distributions.Normal(0, 1)\n","    grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n","    grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n","    image_width = digit_size*n\n","    image_height = image_width\n","    image = np.zeros((image_height, image_width))\n","\n","    for i, yi in enumerate(grid_x):\n","        for j, xi in enumerate(grid_y):\n","            z = np.array([[xi, yi]])\n","            x_decoded = model.sample(z)\n","            digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n","            image[i * digit_size: (i + 1) * digit_size,\n","                j * digit_size: (j + 1) * digit_size] = digit.numpy()\n","\n","    plt.figure(figsize=(10, 10))\n","    plt.imshow(image, cmap='Greys_r')\n","    plt.axis('Off')\n","    plt.show();\n","    return\n","\n","def lighten_color(color, amount=0.5):\n","    \"\"\"\n","    Lightens the given color by multiplying (1-luminosity) by the given amount.\n","    Input can be matplotlib color string, hex string, or RGB tuple.\n","\n","    Examples:\n","    >> lighten_color('g', 0.3)\n","    >> lighten_color('#F034A3', 0.6)\n","    >> lighten_color((.3,.55,.1), 0.5)\n","    \"\"\"\n","    import matplotlib.colors as mc\n","    import colorsys\n","    try:\n","        c = mc.cnames[color]\n","    except:\n","        c = color\n","    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n","    return colorsys.hls_to_rgb(c[0], 1 - amount * (1 - c[1]), c[2])\n","\n","\n","def plot_training(vae_dict, metric='neg_elbo'):\n","  for key, vae in vae_dict.items():\n","      metric_df = pd.concat([vae.metrics_df, vae.val_metrics_df], axis=1)\n","      x = metric_df['epoch']\n","      y = metric_df[f'train_{metric}']\n","      plt.plot(x, y, label=f'Training {metric}')\n","      y = metric_df[f'val_{metric}']\n","      plt.plot(x, y, label=f'Validation {metric}')\n","      plt.xticks(x)\n","      plt.legend()\n","      plt.xlabel('Epoch')\n","      plt.ylabel(f'{metric}')\n","      plt.title(f'Training and Validation {metric} vs. Epochs')\n","      plt.show()\n","  return"]},{"cell_type":"markdown","metadata":{"id":"-ldAtHFcmcOY"},"source":["## Training and Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3a5xb7PdmcX4"},"outputs":[],"source":["def train_VAE(loss_coupling, \n","              z_dim, \n","              beta, \n","              p_std, \n","              analytic_kl, \n","              n_epoch, \n","              n_epoch_display, \n","              datasets, \n","              random_seed, \n","              model_path, \n","              test_name,\n","              show_display,\n","              early_stop\n","              ):\n","  '''This function runs an experiment with the passed in parameters.\n","\n","Inputs\n","  ------\n","  loss_coupling : float\n","    The coupling loss parameter.\n","  z_dim : int\n","    A positive integer for the dimensionality of the latent space.\n","  beta : float\n","    The weight to place on the coupled divergence in the coupled ELBO.\n","  p_std : float\n","    The prior distribution's scale parameter.\n","  analytic_kl : bool\n","    Whether or not to use the analytical coupled divergence.\n","  n_epoch : int\n","    The number of epochs to train for.\n","  n_epoch_display : int\n","    The number of epochs to wait before displaying plots after an epoch.\n","  datasets : collections defaultdict\n","    A collection of datasets.\n","  random_seed : int\n","    A random seed.\n","  model_path : pathlib Path\n","    The root path of the output.\n","  test_name : str\n","    Name of the dataset.\n","  show_display : bool\n","    Whether or not to display plots while training.\n","  early_stop : int\n","    Max number of epochs to run since the last improvement.\n","\n","  Returns\n","  -------\n","  tuple: \n","    display_name : str\n","      A name associated with the experiment.\n","    vae : VAE\n","      A trained VAE object.\n","\n","  '''\n","  \n","  print(test_name)\n","  #Setting parameter string for files to be named    \n","  parameter_str = '_beta_' + str(beta) + '_zdim_' + str(z_dim) + \\\n","                    '_p_std_' + str(p_std) + '_coupling_' + str(loss_coupling) +\\\n","                    '_seed_'+str(random_seed)\n","  print('parameter_str:\\t', parameter_str)\n","\n","  # display_name = 'original' if test_name == 'mnist' else test_name.split('/')[-1]\n","  display_name = test_name.split('/')[-1]\n","  \n","  display_path = model_path / display_name\n","  my_vae = VAE(\n","      z_dim=z_dim,\n","      beta=beta,\n","      p_std=p_std,\n","      seed=random_seed,\n","      loss_coupling=tf.cast(loss_coupling, tf.float64),\n","      analytic_kl=analytic_kl,\n","      dtype='float64',\n","      display_path=display_path\n","      )\n","  my_vae.train(\n","      train_dataset=datasets['mnist']['train'],\n","      val_dataset=datasets['mnist']['val'],\n","      val_label=datasets['mnist']['val_label'],\n","      n_epoch=n_epoch,\n","      n_epoch_display=n_epoch_display,\n","      model_path=model_path,\n","      show_display=show_display,\n","      early_stop=early_stop\n","      )\n","  #Save to metrics tables.csv\n","  full_metrics_df = pd.concat([my_vae.metrics_df, my_vae.val_metrics_df], axis=1)\n","  full_metrics_df.to_csv(\n","      f\"{display_path}/metrics/table_{parameter_str}.csv\", \n","      index=False\n","      )\n","  return display_name, my_vae\n","\n","def train_VAEs(loss_coupling_vals, \n","               z_dim_vals, \n","               beta, \n","               p_std, \n","               analytic_kl, \n","               n_epoch, \n","               n_epoch_display, \n","               datasets,\n","               datasets_names,\n","               random_seed, \n","               model_path,\n","               show_display,\n","               early_stop\n","               ):\n","  '''This function runs experiments with the passed in parameters and \n","  parameter lists.\n","\n","  Inputs\n","  ------\n","  loss_coupling_vals : list\n","    A list of floats for the coupling loss parameter.\n","  z_dim_vals : list\n","    A list of positive integers for the dimensionality of the latent space.\n","  beta : float\n","    The weight to place on the coupled divergence in the coupled ELBO.\n","  p_std : float\n","    The prior distribution's scale parameter.\n","  analytic_kl : bool\n","    Whether or not to use the analytical coupled divergence.\n","  n_epoch : int\n","    The number of epochs to train for.\n","  n_epoch_display : int\n","    The number of epochs to wait before displaying plots after an epoch.\n","  datasets : collections defaultdict\n","    A collection of datasets.\n","  datsets_names : list\n","    A list of the dataset names.\n","  random_seed : int\n","    A random seed.\n","  model_path : pathlib Path\n","    The root path of the output.\n","  show_display : bool\n","    Whether or not to display plots while training.\n","  early_stop : int\n","    Max number of epochs to run since the last improvement.\n","\n","  Returns\n","  -------\n","  vae_dict : dict\n","    A dict of the VAEs for each data set type.\n","\n","  '''\n","  vae_dict = {}\n","  for loss_coupling in loss_coupling_vals:\n","    for z_dim in z_dim_vals:\n","      for test_name in datasets_names:\n","\n","        display_name, trained_vae = train_VAE(\n","            loss_coupling=loss_coupling, \n","            z_dim=z_dim,\n","            beta=beta, \n","            p_std=p_std, \n","            analytic_kl=analytic_kl, \n","            n_epoch=n_epoch, \n","            n_epoch_display=n_epoch_display, \n","            datasets=datasets, \n","            random_seed=random_seed, \n","            model_path=model_path, \n","            test_name=test_name,\n","            show_display=show_display,\n","            early_stop=early_stop\n","            )\n","\n","        vae_dict[display_name] = trained_vae\n","  return vae_dict\n","\n","\n","def test_VAE(my_vae,\n","             test_data,\n","             test_labels, \n","             test_path, \n","             test_name,\n","             show_display,\n","             random_seed\n","             ):\n","  '''This function runs an experiment with the passed in parameters.\n","\n","  Inputs\n","  ------\n","  my_vae : VAE\n","    A trained VAE.\n","  \n","  test_path : pathlib Path\n","    The root path of the output.\n","  test_name : str\n","    Name of the dataset.\n","  show_display : bool\n","    Whether or not to display plots while training.\n","\n","  Returns\n","  -------\n","  tuple: \n","    display_name : str\n","      A name associated with the experiment.\n","    vae : VAE\n","      A trained VAE object.\n","\n","  '''\n","  \n","  print(test_name)\n","  #Setting parameter string for files to be named    \n","  parameter_str = '_beta_' + str(my_vae.beta) + '_zdim_' + str(my_vae.z_dim) + \\\n","                    '_p_std_' + str(my_vae.p_std) + '_coupling_' + str(round(my_vae.loss_coupling.numpy(), 10)) +\\\n","                    '_seed'+str(random_seed)\n","  print('parameter_str:\\t', parameter_str)\n","\n","  # display_name = 'original' if test_name == 'mnist' else test_name.split('/')[-1]\n","  display_name = test_name.split('/')[-1]\n","  print(display_name)\n","  \n","  display_path = test_path / display_name\n","  print(display_path)\n","  #Save to metrics tables.csv\n","  full_metrics_df =   my_vae.test(\n","      test_corrupted=test_data,\n","      test_clean=datasets['mnist_corrupted/identity']['test'],\n","      test_label=test_labels,\n","      save_path=display_path,\n","      show_display=show_display\n","  )\n","  full_metrics_df = pd.DataFrame(full_metrics_df).T\n","  print(f\"Writing {display_path / 'metrics' / f'table_{parameter_str}.csv'}\")\n","  full_metrics_df.to_csv(\n","      display_path / 'metrics' / f\"table_{parameter_str}.csv\", \n","      index=False\n","      )\n","  # Calculate the KL probabilities.\n","  #kl_probs =tf.exp(-tf.concat(kl_values, axis=0))\n","\n","  return #kl_probs\n","\n","\n","def test_VAE_loop(my_vae,\n","                  datasets, \n","                  test_path, \n","                  show_display,\n","                  random_seed\n","                  ):\n","  '''\n","  datasets : collections defaultdict\n","    A collection of datasets.\n","  '''\n","  for key in datasets.keys():\n","    test_data = datasets[key]['test']\n","    test_labels = datasets[key]['test_label']\n","\n","    test_VAE(my_vae,\n","             test_data,\n","             test_labels, \n","             test_path=test_path, \n","             test_name=key,\n","             show_display=show_display,\n","             random_seed=random_seed\n","             )\n","\n","  return"]},{"cell_type":"markdown","metadata":{"id":"zsyPIq0MIcEC"},"source":["# Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cZZ62PjekJvo"},"outputs":[],"source":["# Random seed is the microsecond for current time.\n","random_seed = datetime.now().microsecond\n","\n","# Set the training and testing batch sizes.\n","BATCH_SIZE_TRAIN = 128\n","BATCH_SIZE_TEST = 5000\n","# The number at which to split training sets and validation set, with training \n","# set size = mnist_split, and validation = 60000 - mnist_split.\n","mnist_split = '55000' \n","\n","# List the different mnist data sets to use. The first should be the training\n","# dataset.\n","corrupted_names = ['identity', 'motion_blur', 'translate', 'rotate']\n","datasets_names = ['mnist'] + [\n","  f'mnist_corrupted/{corrupted_name}' for corrupted_name in corrupted_names\n","  ]\n","\"\"\"\n","# Download the data sets.\n","datasets = get_datasets(\n","    datasets_names, \n","    BATCH_SIZE_TRAIN, \n","    BATCH_SIZE_TEST, \n","    mnist_split, \n","    random_seed\n","    )\n","\"\"\"\n","training_datasets = ['train/']\n","testing_datasets = ['test/' + name.split('/')[1] for name in datasets_names[1:]]\n","#training_datasets = [datasets_names[0]]\n","\n","# Get the list of keys from the datasets dict.\n","testing_datasets = list(datasets.keys())\n","# Drop 'mnist', so only the corrupted dataset names remain.\n","testing_datasets.remove('mnist')\n","# Create an empty dictionary to hold only the testing datasets.\n","testing_datasets_dict = dict()\n","# Loop through the corrupted dataset names.\n","for dataset in testing_datasets:\n","  # Add the corrupted data set to the new dictionary.\n","  testing_datasets_dict[dataset] = datasets[dataset]\n"]},{"cell_type":"markdown","metadata":{"id":"0929v8fLIlQp"},"source":["# Set Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dEZGlQtm-jXl"},"outputs":[],"source":["check_gpu_availibility()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yvQilJMlcH4"},"outputs":[],"source":["###\n","# VAE Initializing Parameters\n","###\n","\n","# Latent dim, set the dimensionality of the latent space.\n","z_dim_vals = [20]#[2, 4, 8, 16]\n","# Whether to use the analytical coupled divergence, or approximate.\n","analytic_kl = True\n","# Set the weight to place on the coupled dsivergence.\n","beta = 1.\n","# Set the standard deviation of the prior distribution.\n","p_std = 1.\n","# Set the loss coupling.\n","loss_coupling_vals = [0.1]#[0.0, 0.025, 0.05, 0.075]\n","# Set the number of base filters in the CNN.\n","n_filter_base = 128\n","# Set the learning rate for the Adam optimizer.\n","learning_rate = 0.0005\n","\n","\n","###\n","# VAE Training Parameters\n","###\n","\n","# Set the number of epochs to display.\n","n_epoch = 100\n","# Set the number of epochs before plots are displayed.\n","n_epoch_display = 1\n","# Whether or not to display plots while training.\n","show_display = False\n","display_sample = True\n","\n","\n","###\n","# Setting Paths\n","###\n","\n","# Set the version of the code being ran.\n","version = 'v7_Z1X1'\n","# Create the root path where the data will be stored.\n","#save_path = Path(\n","#    f'gdrive/My Drive/Colab Notebooks/coupled_vae/vae/output/{version}/'\n","#    )\n","save_path = Path(\n","    f'gdrive/My Drive/Colab Notebooks/coupled_vae/{version}/'\n","    )\n","# If the path does not exist, make it.\n","save_path.mkdir(parents=True, exist_ok=True)\n","\n","# Set the dirctory where run results will be saved.\n","model_path = save_path / str(random_seed)\n","model_path.mkdir(parents=True, exist_ok=True)\n","\n","# Create the folders for this run in the google drive. It will not override \n","# existing version and seed folders\n","create_gdrive_output_folders(model_path,\n","                             img_folders=corrupted_names)\n","\n","# Save the parameters in a dict.\n","param_dict = {\n","  'random_seed': random_seed,\n","  'z_dim_vals': z_dim_vals,\n","  'analytic_kl': analytic_kl,\n","  'beta': beta,\n","  'p_std': p_std,\n","  'loss_coupling_vals': loss_coupling_vals,\n","  'n_filter_base': n_filter_base,\n","  'learning_rate': learning_rate,\n","  'n_epoch': n_epoch,\n","  'n_epoch_display': n_epoch_display,\n","  'train_batch_size': BATCH_SIZE_TRAIN,\n","  'test_batch_size': BATCH_SIZE_TEST,\n","  'val_split': mnist_split,\n","  'datasets': datasets_names,\n","  'show_display': show_display,\n","  'display_sample': show_display,\n","  'model_path': model_path\n","}\n","\n","# Set the path for the experiment tracking CSV file.\n","experiment_tracker_path = save_path / 'experiment_tracker.csv'\n","# Update the file.\n","update_experiments(param_dict, experiment_tracker_path)\n","\n","# Set the training and testing paths.\n","training_path = model_path / 'train'\n","testing_path = model_path / 'test'"]},{"cell_type":"markdown","metadata":{"id":"asZBIWqiUmZ1"},"source":["# Train VAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-n06HDY5y3xj"},"outputs":[],"source":["early_stop = 3\n","vae_dict = train_VAEs(\n","    loss_coupling_vals, \n","    z_dim_vals, \n","    beta,\n","    p_std, \n","    analytic_kl, \n","    n_epoch,\n","    n_epoch_display, \n","    datasets,\n","    training_datasets,\n","    random_seed, \n","    training_path,\n","    show_display=show_display,\n","    early_stop=early_stop\n","    )"]},{"cell_type":"markdown","metadata":{"id":"UgxL7BoGGT4B"},"source":["# Plot Training Performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xr1A1yYx3eYu"},"outputs":[],"source":["# Plot the latent space.\n","if z_dim_vals == [2]:\n","  for vae_key in vae_dict.keys():\n","    print(f'Latent Space for {vae_key}')\n","    plot_latent_images(vae_dict[vae_key].model, n=15, digit_size=28)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"joC0eJDJNAwA"},"outputs":[],"source":["plot_training(vae_dict, metric='neg_elbo')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ozIkzP5MyNw"},"outputs":[],"source":["plot_training(vae_dict, metric='recon_loss')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pur9pNROMzCc"},"outputs":[],"source":["plot_training(vae_dict, metric='coupled_div')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ydKFBgXCf5Z4"},"outputs":[],"source":["best_model_epoch = vae_dict[''].val_metrics_df.loc[\n","  vae_dict[''].val_metrics_df['val_neg_elbo'] == vae_dict[''].val_metrics_df['val_neg_elbo'].min()\n","].index.values + 1\n","\n","print(f'The best model was saved at epoch {best_model_epoch[0]}.')"]},{"cell_type":"markdown","metadata":{"id":"QxGq2nAOGXOq"},"source":["# Test"]},{"cell_type":"code","source":["vae.__dict__.keys()"],"metadata":{"id":"BCXY8UvW3E9W"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-2PJr9Qo_1m"},"outputs":[],"source":["vae = vae_dict['']\n","vae = VAE(z_dim=vae.__dict__['z_dim'], \n","          beta=vae.__dict__['beta'], \n","          p_std=vae.__dict__['p_std'], \n","          loss_coupling=vae.__dict__['loss_coupling'],\n","          analytic_kl=vae.__dict__['analytic_kl'], \n","          dtype=vae.__dict__['dtype'], \n","          display_path=vae.__dict__['display_path']\n",")\n","\n","# Load the best model by validation set performance from the checkpoints.\n","vae.model.load_weights(str(model_path) + '/train/cp.ckpt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cH7Irj6K2KYO"},"outputs":[],"source":["# Get the list of keys from the datasets dict.\n","testing_datasets = list(datasets.keys())\n","# Drop 'mnist', so only the corrupted dataset names remain.\n","testing_datasets.remove('mnist')\n","# Create an empty dictionary to hold only the testing datasets.\n","testing_datasets_dict = dict()\n","# Loop through the corrupted dataset names.\n","for dataset in testing_datasets:\n","  # Add the corrupted data set to the new dictionary.\n","  testing_datasets_dict[dataset] = datasets[dataset]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yukb92gD329B"},"outputs":[],"source":["test_VAE_loop(\n","    my_vae=vae,\n","    datasets=testing_datasets_dict, \n","    test_path=testing_path, \n","    show_display=True,\n","    random_seed=random_seed\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GoePj7v3wxGC"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":["k6ukXDdxjdP8","_2y2JXNJgN6Z","9rU_nMnnjlKW","Wt4eHh5pl04k","Vtxx6SXPl-xy","Qxh11GiRmBqW","EfocmOTImRFs","0-XA_4yGj2oG","Vagw09Fqnq_O","YASkxNpHnOK1","-ldAtHFcmcOY"],"name":"coupled_vae_v7.ipynb","provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}